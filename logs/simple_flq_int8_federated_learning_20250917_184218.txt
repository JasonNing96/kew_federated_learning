# FLQ实验记录 - flq_int8_federated_learning
# 时间: 2025-09-17 18:42:18
# 配置: workers=10, rounds=50, epochs=1, lr=0.001
# 格式: round, global_acc, global_loss, avg_train_acc, avg_train_loss, comm_bits, comm_workers, comm_rate
--------------------------------------------------------------------------------
  0, 0.0830, 2.3059, 0.9297, 0.9092, 24034080, 10, 1.000
  1, 0.0845, 2.3074, 0.9300, 0.9067,        0,  0, 0.000
  2, 0.0865, 2.3071, 0.9309, 0.9076,        0,  0, 0.000
  3, 0.0920, 2.3055, 0.9321, 0.9039,        0,  0, 0.000
  4, 0.0965, 2.3062, 0.9301, 0.9083,        0,  0, 0.000
  5, 0.0985, 2.3057, 0.9296, 0.9088,        0,  0, 0.000
  6, 0.0965, 2.3059, 0.9306, 0.9057,        0,  0, 0.000
  7, 0.0935, 2.3076, 0.9301, 0.9133,        0,  0, 0.000
  8, 0.0945, 2.3046, 0.9291, 0.9148,        0,  0, 0.000
  9, 0.0840, 2.3075, 0.9295, 0.9144,        0,  0, 0.000
 10, 0.0800, 2.3077, 0.9300, 0.9117,        0,  0, 0.000
 11, 0.0805, 2.3086, 0.9303, 0.9112,        0,  0, 0.000
 12, 0.0960, 2.3069, 0.9322, 0.9074,        0,  0, 0.000
 13, 0.0960, 2.3048, 0.9306, 0.9131,        0,  0, 0.000
 14, 0.0925, 2.3068, 0.9278, 0.9176,        0,  0, 0.000
 15, 0.0925, 2.3071, 0.9308, 0.9091,        0,  0, 0.000
 16, 0.0900, 2.3079, 0.9303, 0.9092,        0,  0, 0.000
 17, 0.0895, 2.3048, 0.9296, 0.9095,        0,  0, 0.000
 18, 0.0885, 2.3086, 0.9304, 0.9072,        0,  0, 0.000
 19, 0.0860, 2.3085, 0.9300, 0.9059,        0,  0, 0.000
 20, 0.0925, 2.3058, 0.9303, 0.9160,        0,  0, 0.000
 21, 0.0855, 2.3049, 0.9303, 0.9148,        0,  0, 0.000
 22, 0.0820, 2.3087, 0.9304, 0.9094,        0,  0, 0.000
 23, 0.0870, 2.3066, 0.9298, 0.9111,        0,  0, 0.000
 24, 0.0800, 2.3055, 0.9290, 0.9094,        0,  0, 0.000
 25, 0.0860, 2.3081, 0.9289, 0.9138,        0,  0, 0.000
 26, 0.0820, 2.3058, 0.9306, 0.9078,        0,  0, 0.000
 27, 0.0945, 2.3074, 0.9282, 0.9114,        0,  0, 0.000
 28, 0.0925, 2.3064, 0.9301, 0.9118,        0,  0, 0.000
 29, 0.0745, 2.3085, 0.9295, 0.9070,        0,  0, 0.000
 30, 0.0880, 2.3050, 0.9306, 0.9106,        0,  0, 0.000
 31, 0.0915, 2.3073, 0.9305, 0.9114,        0,  0, 0.000
 32, 0.0885, 2.3049, 0.9306, 0.9084,        0,  0, 0.000
 33, 0.0850, 2.3078, 0.9291, 0.9126,        0,  0, 0.000
 34, 0.0875, 2.3058, 0.9302, 0.9093,        0,  0, 0.000
 35, 0.0905, 2.3068, 0.9284, 0.9196,        0,  0, 0.000
 36, 0.0865, 2.3044, 0.9299, 0.9095,        0,  0, 0.000
 37, 0.0855, 2.3065, 0.9315, 0.9043,        0,  0, 0.000
 38, 0.0825, 2.3057, 0.9294, 0.9098,        0,  0, 0.000
 39, 0.0970, 2.3053, 0.9303, 0.9098,        0,  0, 0.000
 40, 0.0865, 2.3061, 0.9305, 0.9134,        0,  0, 0.000
 41, 0.0775, 2.3067, 0.9300, 0.9141,        0,  0, 0.000
 42, 0.0855, 2.3073, 0.9305, 0.9097,        0,  0, 0.000
 43, 0.0965, 2.3060, 0.9297, 0.9084,        0,  0, 0.000
 44, 0.0745, 2.3061, 0.9307, 0.9162,        0,  0, 0.000
 45, 0.0995, 2.3053, 0.9298, 0.9121,        0,  0, 0.000
 46, 0.0765, 2.3085, 0.9298, 0.9139,        0,  0, 0.000
 47, 0.0885, 2.3070, 0.9316, 0.9072,        0,  0, 0.000
 48, 0.0845, 2.3084, 0.9301, 0.9108,        0,  0, 0.000
 49, 0.0940, 2.3085, 0.9300, 0.9104,        0,  0, 0.000
