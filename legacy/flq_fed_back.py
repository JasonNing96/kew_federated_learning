#!/usr/bin/env python3
"""
FLQè”é‚¦å­¦ä¹ å®ç°è„šæœ¬
åŸºäºlegacy/FLQ.pyçš„é‡åŒ–å’Œæ‡’æƒ°èšåˆæœºåˆ¶
æ”¯æŒMNISTå’ŒFashion-MNISTæ•°æ®é›†çš„çœŸå®è”é‚¦å­¦ä¹ è¿‡ç¨‹
"""

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import time
import json
import copy
import os
from typing import List, Tuple, Dict, Any
from sklearn.preprocessing import QuantileTransformer

# ---------------------
# å…¨å±€é…ç½®å‚æ•°
# ---------------------
# è”é‚¦å­¦ä¹ åŸºç¡€é…ç½®
NUM_WORKERS = 10           # å®¢æˆ·ç«¯æ•°é‡ M
NUM_ROUNDS = 800          # è®­ç»ƒè½®æ¬¡ Iter
BATCH_SIZE = 64           # æ‰¹æ¬¡å¤§å°
LEARNING_RATE = 0.02      # å­¦ä¹ ç‡ alpha
SEED = 1234               # éšæœºç§å­

# FLQé‡åŒ–é…ç½®
QUANTIZATION_BITS = 4     # é‡åŒ–ä½æ•° b
REGULARIZATION_COEF = 0.01  # L2æ­£åˆ™åŒ–ç³»æ•° cl

# FLQæ‡’æƒ°èšåˆå‚æ•°
LAZY_D = 10               # å†å²çª—å£å¤§å° D
FORCE_COMM_PERIOD = 100   # å¼ºåˆ¶é€šä¿¡å‘¨æœŸ C
WEIGHT_DECAY = 0.8        # æƒé‡è¡°å‡ç³»æ•° ck
BETA_PARAM = 0.001        # Betaå‚æ•°

# æ•°æ®é›†é…ç½®
DATASET_TYPE = "mnist"    # "mnist" æˆ– "fashion_mnist"

print(f"ğŸ¯ FLQè”é‚¦å­¦ä¹ é…ç½®:")
print(f"  å®¢æˆ·ç«¯æ•°: {NUM_WORKERS}")
print(f"  è®­ç»ƒè½®æ¬¡: {NUM_ROUNDS}")
print(f"  é‡åŒ–ä½æ•°: {QUANTIZATION_BITS}")
print(f"  æ•°æ®é›†: {DATASET_TYPE}")

# è®¾ç½®éšæœºç§å­
np.random.seed(SEED)
tf.random.set_seed(SEED)

# è®¾å¤‡é…ç½®
print(f"  ä½¿ç”¨è®¾å¤‡: {'GPU' if tf.config.list_physical_devices('GPU') else 'CPU'}")

# ---------------------
# æ ¸å¿ƒå·¥å…·å‡½æ•°ï¼ˆæ¥è‡ªFLQ.pyï¼‰
# ---------------------
def gradtovec(grad):
    """å°†æ¢¯åº¦åˆ—è¡¨è½¬æ¢ä¸ºä¸€ç»´å‘é‡"""
    vec = np.array([])
    le = len(grad)
    for i in range(0, le):
        a = grad[i]
        b = a.numpy()

        if len(a.shape) == 2:
            da = int(a.shape[0])
            db = int(a.shape[1])
            b = b.reshape(da * db)
        vec = np.concatenate((vec, b), axis=0)
    return vec

def vectograd(vec, grad):
    """å°†ä¸€ç»´å‘é‡è½¬æ¢å›æ¢¯åº¦æ ¼å¼"""
    le = len(grad)
    for i in range(0, le):
        a = grad[i]
        b = a.numpy()
        if len(a.shape) == 2:
            da = int(a.shape[0])
            db = int(a.shape[1])
            c = vec[0:da * db]
            c = c.reshape(da, db)
            lev = len(vec)
            vec = vec[da * db:lev]
        else:
            da = int(a.shape[0])
            c = vec[0:da]
            lev = len(vec)
            vec = vec[da:lev]
        grad[i] = 0 * grad[i] + c
    return grad

def quantd(vec, v2, b):
    """FLQé‡åŒ–å‡½æ•°ï¼ˆæ¥è‡ªåŸå§‹ä»£ç ï¼‰"""
    n = len(vec)
    r = max(abs(vec - v2))
    if r == 0:
        return vec.copy()
    delta = r / (np.floor(2 ** b) - 1)
    quantv = v2 - r + 2 * delta * np.floor((vec - v2 + r + delta) / (2 * delta))
    return quantv

# ---------------------
# æ•°æ®é›†åŠ è½½å’Œåˆ†å‰²
# ---------------------
def load_dataset(dataset_type="mnist"):
    """åŠ è½½æŒ‡å®šæ•°æ®é›†"""
    if dataset_type == "mnist":
        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
    elif dataset_type == "fashion_mnist":
        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†ç±»å‹: {dataset_type}")
    
    # æ•°æ®é¢„å¤„ç†
    x_train = x_train / 255.0
    x_test = x_test / 255.0
    
    print(f"ğŸ“¥ å·²åŠ è½½{dataset_type}æ•°æ®é›†:")
    print(f"  è®­ç»ƒé›†: {x_train.shape}, æµ‹è¯•é›†: {x_test.shape}")
    
    return (x_train, y_train), (x_test, y_test)

def split_data_for_workers(x_train, y_train, num_workers):
    """ä¸ºè”é‚¦å­¦ä¹ å®¢æˆ·ç«¯åˆ†å‰²æ•°æ®"""
    num_samples = len(x_train)
    samples_per_worker = num_samples // num_workers
    
    worker_data = []
    for m in range(num_workers):
        start_idx = m * samples_per_worker
        end_idx = (m + 1) * samples_per_worker
        
        worker_x = x_train[start_idx:end_idx]
        worker_y = y_train[start_idx:end_idx]
        
        # åˆ›å»ºTensorFlowæ•°æ®é›†
        dataset = tf.data.Dataset.from_tensor_slices(
            (tf.cast(worker_x[..., tf.newaxis], tf.float32),
             tf.cast(worker_y, tf.int64))
        )
        dataset = dataset.batch(samples_per_worker)
        worker_data.append(dataset)
        
        # ç»Ÿè®¡æ¯ä¸ªå®¢æˆ·ç«¯çš„ç±»åˆ«åˆ†å¸ƒ
        unique, counts = np.unique(worker_y, return_counts=True)
        print(f"  å®¢æˆ·ç«¯{m}: {len(worker_y)}æ ·æœ¬, ç±»åˆ«åˆ†å¸ƒ {dict(zip(unique, counts))}")
    
    return worker_data

# ---------------------
# æ¨¡å‹å®šä¹‰ï¼ˆåŸºäºFLQ.pyçš„é‡åŒ–æ¨¡å‹ï¼‰
# ---------------------
def create_flq_model():
    """åˆ›å»ºFLQé‡åŒ–æ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆæœ¬é¿å…åºåˆ—åŒ–é—®é¢˜ï¼‰"""
    regularizer = tf.keras.regularizers.L2(l2=0.9)
    
    model = tf.keras.models.Sequential([
        tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(10, activation='softmax', kernel_regularizer=regularizer)
    ])
    
    model.compile(
        optimizer=tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

# ---------------------
# FLQè”é‚¦å­¦ä¹ æœåŠ¡å™¨
# ---------------------
class FLQFederatedServer:
    """FLQè”é‚¦å­¦ä¹ æœåŠ¡å™¨ - å®ç°æ‡’æƒ°èšåˆæœºåˆ¶"""
    
    def __init__(self, model, num_workers):
        self.global_model = model
        self.num_workers = num_workers
        
        # è®¡ç®—æ¨¡å‹å‚æ•°ç»´åº¦
        self.param_dim = sum(np.prod(var.shape) for var in model.trainable_variables)
        
        # FLQæ‡’æƒ°èšåˆçŠ¶æ€
        self.worker_clocks = np.zeros(num_workers)
        self.worker_errors = np.zeros(num_workers)
        self.worker_error_hats = np.zeros(num_workers)
        self.worker_last_gradients = np.zeros((num_workers, self.param_dim))
        self.communication_indicators = np.zeros((num_workers, NUM_ROUNDS))
        
        # å†å²å‚æ•°å˜åŒ–
        self.parameter_history = np.zeros((self.param_dim, NUM_ROUNDS))
        self.ksi_weights = self._initialize_ksi_weights()
        
        # èšåˆæ¢¯åº¦
        self.aggregated_gradient = np.zeros(self.param_dim)
        
        print(f"ğŸ›ï¸ FLQæœåŠ¡å™¨åˆå§‹åŒ–å®Œæˆï¼Œæ¨¡å‹å‚æ•°ç»´åº¦: {self.param_dim}")
    
    def _initialize_ksi_weights(self):
        """åˆå§‹åŒ–ksiæƒé‡çŸ©é˜µï¼ˆæ¥è‡ªåŸå§‹ä»£ç ï¼‰"""
        ksi = np.ones((LAZY_D, LAZY_D + 1))
        for i in range(LAZY_D + 1):
            if i == 0:
                ksi[:, i] = np.ones(LAZY_D)
            elif i <= LAZY_D and i > 0:
                ksi[:, i] = 1 / i * np.ones(LAZY_D)
        return WEIGHT_DECAY * ksi
    
    def check_communication_condition(self, worker_id, round_id, gradient, quantization_error):
        """æ£€æŸ¥FLQæ‡’æƒ°èšåˆé€šä¿¡æ¡ä»¶"""
        # å¼ºåˆ¶é€šä¿¡æ¡ä»¶
        if self.worker_clocks[worker_id] >= FORCE_COMM_PERIOD:
            return True
        
        # è®¡ç®—æ¢¯åº¦å·®å€¼ dL[m] = gr[m] - mgr[m]
        gradient_diff = gradient - self.worker_last_gradients[worker_id]
        gradient_diff_norm_sq = np.sum(gradient_diff ** 2)
        
        # è®¡ç®—åŠ¨æ€é˜ˆå€¼ me[m]
        me_threshold = self._calculate_dynamic_threshold(worker_id, round_id)
        
        # FLQé€šä¿¡æ¡ä»¶åˆ¤æ–­
        threshold = (1 / (LEARNING_RATE ** 2 * self.num_workers ** 2)) * me_threshold + \
                   3 * (quantization_error + self.worker_error_hats[worker_id])
        
        should_communicate = gradient_diff_norm_sq >= threshold
        
        return should_communicate
    
    def _calculate_dynamic_threshold(self, worker_id, round_id):
        """è®¡ç®—åŠ¨æ€é˜ˆå€¼me[m]ï¼ˆæ¥è‡ªåŸå§‹ä»£ç é€»è¾‘ï¼‰"""
        me_value = 0.0
        
        for d in range(LAZY_D):
            if round_id - d >= 0:
                if round_id <= LAZY_D:
                    weight = self.ksi_weights[d, round_id] if round_id < self.ksi_weights.shape[1] else 0.0
                else:
                    weight = self.ksi_weights[d, LAZY_D] if LAZY_D < self.ksi_weights.shape[1] else 0.0
                
                if round_id - d < self.parameter_history.shape[1]:
                    param_change = self.parameter_history[:, round_id - d]
                    me_value += weight * np.sum(param_change ** 2)
        
        return me_value
    
    def receive_worker_update(self, worker_id, round_id, gradient, quantized_gradient, quantization_error):
        """æ¥æ”¶å®¢æˆ·ç«¯æ›´æ–°å¹¶åº”ç”¨æ‡’æƒ°èšåˆ"""
        # æ£€æŸ¥é€šä¿¡æ¡ä»¶
        should_communicate = self.check_communication_condition(worker_id, round_id, quantized_gradient, quantization_error)
        
        # è®°å½•é€šä¿¡æŒ‡ç¤ºå™¨
        self.communication_indicators[worker_id, round_id] = 1 if should_communicate else 0
        
        if should_communicate:
            # è®¡ç®—æ¢¯åº¦å·®å€¼å¹¶ç´¯ç§¯åˆ°èšåˆæ¢¯åº¦
            gradient_diff = quantized_gradient - self.worker_last_gradients[worker_id]
            self.aggregated_gradient += gradient_diff
            
            # æ›´æ–°å®¢æˆ·ç«¯çŠ¶æ€
            self.worker_last_gradients[worker_id] = quantized_gradient.copy()
            self.worker_error_hats[worker_id] = quantization_error
            self.worker_clocks[worker_id] = 0
            
            print(f"ğŸ“¡ å®¢æˆ·ç«¯{worker_id} å‚ä¸é€šä¿¡")
        else:
            # å¢åŠ æ—¶é’Ÿè®¡æ•°
            self.worker_clocks[worker_id] += 1
            print(f"â¸ï¸ å®¢æˆ·ç«¯{worker_id} è·³è¿‡é€šä¿¡ (clock: {self.worker_clocks[worker_id]})")
        
        return should_communicate
    
    def apply_global_update(self, round_id):
        """åº”ç”¨å…¨å±€æ¨¡å‹æ›´æ–°"""
        # è·å–å½“å‰æ¨¡å‹å‚æ•°
        current_params = gradtovec(self.global_model.trainable_variables)
        
        # åº”ç”¨èšåˆæ¢¯åº¦æ›´æ–°
        updated_params = current_params + self.aggregated_gradient
        
        # æ›´æ–°æ¨¡å‹å‚æ•°
        updated_grads = vectograd(self.aggregated_gradient, self.global_model.trainable_variables)
        optimizer = tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE)
        optimizer.apply_gradients(zip(updated_grads, self.global_model.trainable_variables))
        
        # è®°å½•å‚æ•°å˜åŒ–å†å²
        if round_id < NUM_ROUNDS:
            param_change = updated_params - current_params
            self.parameter_history[:, round_id] = param_change
        
        # é‡ç½®èšåˆæ¢¯åº¦
        self.aggregated_gradient = np.zeros(self.param_dim)
        
        # ç»Ÿè®¡é€šä¿¡å®¢æˆ·ç«¯æ•°é‡
        communicating_workers = np.sum(self.communication_indicators[:, round_id])
        print(f"ğŸ”„ ç¬¬{round_id}è½®æ›´æ–°å®Œæˆï¼Œé€šä¿¡å®¢æˆ·ç«¯: {int(communicating_workers)}/{self.num_workers}")

# ---------------------
# FLQè”é‚¦å­¦ä¹ å®¢æˆ·ç«¯
# ---------------------
class FLQFederatedClient:
    """FLQè”é‚¦å­¦ä¹ å®¢æˆ·ç«¯"""
    
    def __init__(self, client_id, dataset, global_model):
        self.client_id = client_id
        self.dataset = dataset
        self.local_model = tf.keras.models.clone_model(global_model)
        self.local_model.compile(
            optimizer=tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        print(f"ğŸ‘¤ å®¢æˆ·ç«¯{client_id}åˆå§‹åŒ–å®Œæˆ")
    
    def local_training(self, global_model):
        """æœ¬åœ°è®­ç»ƒï¼ˆå•è½®epochï¼‰"""
        # åŒæ­¥å…¨å±€æ¨¡å‹å‚æ•°
        self.local_model.set_weights(global_model.get_weights())
        initial_weights = gradtovec(self.local_model.trainable_variables)
        
        # æœ¬åœ°è®­ç»ƒ
        for batch_idx, (images, labels) in enumerate(self.dataset.take(1)):
            with tf.GradientTape() as tape:
                logits = self.local_model(images, training=True)
                loss_value = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True))
                
                # æ·»åŠ L2æ­£åˆ™åŒ–
                l2_loss = 0
                for var in self.local_model.trainable_variables:
                    l2_loss += REGULARIZATION_COEF * tf.nn.l2_loss(var)
                total_loss = loss_value + l2_loss
            
            # è®¡ç®—æ¢¯åº¦
            grads = tape.gradient(total_loss, self.local_model.trainable_variables)
            gradient_vector = gradtovec(grads)
            
            print(f"  å®¢æˆ·ç«¯{self.client_id}: loss={total_loss.numpy():.4f}")
            
            return gradient_vector, total_loss.numpy()
    
    def apply_quantization(self, gradient, reference_gradient):
        """åº”ç”¨FLQé‡åŒ–ï¼ˆä½¿ç”¨åŸå§‹quantdå‡½æ•°ï¼‰"""
        quantized_gradient = quantd(gradient, reference_gradient, QUANTIZATION_BITS)
        quantization_error = np.sum((quantized_gradient - gradient) ** 2)
        return quantized_gradient, quantization_error

# ---------------------
# ä¸»å®éªŒå‡½æ•°
# ---------------------
def run_flq_federated_learning():
    """è¿è¡ŒFLQè”é‚¦å­¦ä¹ å®éªŒ"""
    print(f"\nğŸš€ å¼€å§‹FLQè”é‚¦å­¦ä¹ å®éªŒ - {DATASET_TYPE.upper()}")
    print("=" * 50)
    
    # 1. åŠ è½½æ•°æ®é›†
    (x_train, y_train), (x_test, y_test) = load_dataset(DATASET_TYPE)
    worker_datasets = split_data_for_workers(x_train, y_train, NUM_WORKERS)
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_dataset = tf.data.Dataset.from_tensor_slices(
        (tf.cast(x_test[..., tf.newaxis], tf.float32),
         tf.cast(y_test, tf.int64))
    )
    test_dataset = test_dataset.batch(len(x_test))
    test_labels_onehot = np.eye(10)[y_test]
    
    # 2. åˆ›å»ºå…¨å±€æ¨¡å‹
    global_model = create_flq_model()
    
    # 3. åˆå§‹åŒ–æœåŠ¡å™¨å’Œå®¢æˆ·ç«¯
    server = FLQFederatedServer(global_model, NUM_WORKERS)
    clients = []
    for i in range(NUM_WORKERS):
        client = FLQFederatedClient(i, worker_datasets[i], global_model)
        clients.append(client)
    
    # 4. è®°å½•å®éªŒæ•°æ®
    loss_history = []
    accuracy_history = []
    communication_costs = []
    
    # 5. å¼€å§‹è”é‚¦å­¦ä¹ è®­ç»ƒ
    start_time = time.time()
    
    for round_id in range(NUM_ROUNDS):
        print(f"\nğŸ”„ ç¬¬{round_id}è½®è”é‚¦å­¦ä¹ ...")
        
        round_loss = 0.0
        round_comm_workers = 0
        
        # æ‰€æœ‰å®¢æˆ·ç«¯å‚ä¸è®­ç»ƒ
        for client in clients:
            # æœ¬åœ°è®­ç»ƒ
            gradient, local_loss = client.local_training(server.global_model)
            round_loss += local_loss
            
            # è·å–å‚è€ƒæ¢¯åº¦è¿›è¡Œé‡åŒ–
            reference_gradient = server.worker_last_gradients[client.client_id]
            quantized_gradient, quantization_error = client.apply_quantization(gradient, reference_gradient)
            
            # ä¸ŠæŠ¥ç»™æœåŠ¡å™¨
            should_communicate = server.receive_worker_update(
                client.client_id, round_id, gradient, quantized_gradient, quantization_error
            )
            
            if should_communicate:
                round_comm_workers += 1
        
        # åº”ç”¨å…¨å±€æ›´æ–°
        server.apply_global_update(round_id)
        
        # è¯„ä¼°å…¨å±€æ¨¡å‹
        avg_loss = round_loss / NUM_WORKERS
        loss_history.append(avg_loss)
        
        # è®¡ç®—æµ‹è¯•å‡†ç¡®ç‡ï¼ˆæ¯10è½®ï¼‰
        if round_id % 10 == 0 or round_id == NUM_ROUNDS - 1:
            test_acc = server.global_model.evaluate(x_test, test_labels_onehot, verbose=0)
            test_accuracy = test_acc[1]
            accuracy_history.append(test_accuracy)
            
            print(f"ğŸ¯ ç¬¬{round_id}è½®: loss={avg_loss:.4f}, test_acc={test_accuracy:.3f}, comm={round_comm_workers}/{NUM_WORKERS}")
        
        # è®°å½•é€šä¿¡å¼€é”€
        total_comm_bits = round_comm_workers * QUANTIZATION_BITS * server.param_dim
        communication_costs.append(total_comm_bits)
    
    training_time = time.time() - start_time
    
    # 6. ç”Ÿæˆå®éªŒæŠ¥å‘Š
    final_loss = loss_history[-1]
    final_accuracy = accuracy_history[-1] if accuracy_history else 0.0
    total_comm_bits = sum(communication_costs)
    avg_comm_rate = np.mean([np.sum(server.communication_indicators[:, r]) / NUM_WORKERS for r in range(NUM_ROUNDS)])
    
    print(f"\nâœ… FLQè”é‚¦å­¦ä¹ å®éªŒå®Œæˆï¼")
    print(f"ğŸ“Š å®éªŒæ€»ç»“:")
    print(f"  è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
    print(f"  æœ€ç»ˆæŸå¤±: {final_loss:.4f}")
    print(f"  æœ€ç»ˆå‡†ç¡®ç‡: {final_accuracy:.3f}")
    print(f"  æ€»é€šä¿¡å¼€é”€: {total_comm_bits:,} bits")
    print(f"  å¹³å‡é€šä¿¡ç‡: {avg_comm_rate:.3f}")
    print(f"  å‹ç¼©æ¯”: {32/QUANTIZATION_BITS:.1f}:1")
    
    # 7. ä¿å­˜ç»“æœ
    results = {
        "experiment_config": {
            "dataset": DATASET_TYPE,
            "num_workers": NUM_WORKERS,
            "num_rounds": NUM_ROUNDS,
            "quantization_bits": QUANTIZATION_BITS,
            "learning_rate": LEARNING_RATE
        },
        "results": {
            "final_loss": float(final_loss),
            "final_accuracy": float(final_accuracy),
            "total_communication_bits": int(total_comm_bits),
            "average_communication_rate": float(avg_comm_rate),
            "training_time": float(training_time),
            "loss_history": [float(x) for x in loss_history],
            "accuracy_history": [float(x) for x in accuracy_history],
            "communication_costs": [int(x) for x in communication_costs]
        }
    }
    
    # ä¿å­˜åˆ°Excelæ–‡ä»¶
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    results_file = f"flq_fed_{DATASET_TYPE}_{timestamp}.xlsx"
    
    os.makedirs("results", exist_ok=True)
    
    # ä½¿ç”¨pandasä¿å­˜Excel
    try:
        import pandas as pd
        
        # åˆ›å»ºExcelæ•°æ®
        excel_data = []
        for i, (loss, cost) in enumerate(zip(loss_history, communication_costs)):
            acc = accuracy_history[i//10] if i % 10 == 0 and i//10 < len(accuracy_history) else None
            excel_data.append({
                'round': i,
                'loss': loss,
                'accuracy': acc,
                'comm_bits': cost
            })
        
        df = pd.DataFrame(excel_data)
        df.to_excel(f"results/{results_file}", index=False)
        print(f"ğŸ“ å®éªŒç»“æœå·²ä¿å­˜åˆ°: results/{results_file}")
    except ImportError:
        print("âš ï¸ pandasæœªå®‰è£…ï¼Œè·³è¿‡Excelä¿å­˜")
    
    # 8. ç»˜åˆ¶ç»“æœå›¾è¡¨
    plot_results(loss_history, accuracy_history, communication_costs, server.communication_indicators)
    
    return results, server

def plot_results(loss_history, accuracy_history, communication_costs, communication_indicators):
    """ç»˜åˆ¶å®éªŒç»“æœå›¾è¡¨"""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    # æŸå¤±æ”¶æ•›æ›²çº¿
    ax1.plot(loss_history, 'b-', linewidth=2)
    ax1.set_xlabel('è®­ç»ƒè½®æ¬¡')
    ax1.set_ylabel('å¹³å‡æŸå¤±')
    ax1.set_title('FLQæŸå¤±æ”¶æ•›æ›²çº¿')
    ax1.grid(True, alpha=0.3)
    
    # å‡†ç¡®ç‡æ›²çº¿
    if accuracy_history:
        eval_rounds = np.arange(0, len(loss_history), 10)
        eval_rounds = eval_rounds[:len(accuracy_history)]
        ax2.plot(eval_rounds, accuracy_history, 'g-o', linewidth=2)
        ax2.set_xlabel('è®­ç»ƒè½®æ¬¡')
        ax2.set_ylabel('æµ‹è¯•å‡†ç¡®ç‡')
        ax2.set_title('FLQå‡†ç¡®ç‡æ›²çº¿')
        ax2.grid(True, alpha=0.3)
    
    # é€šä¿¡å¼€é”€
    cumulative_bits = np.cumsum(communication_costs) / 1e6  # è½¬æ¢ä¸ºMbits
    ax3.plot(cumulative_bits, 'r-', linewidth=2)
    ax3.set_xlabel('è®­ç»ƒè½®æ¬¡')
    ax3.set_ylabel('ç´¯è®¡é€šä¿¡å¼€é”€ (Mbits)')
    ax3.set_title('FLQé€šä¿¡å¼€é”€')
    ax3.grid(True, alpha=0.3)
    
    # é€šä¿¡æ¨¡å¼å¯è§†åŒ–ï¼ˆé€‰æ‹©å‡ ä¸ªå®¢æˆ·ç«¯å±•ç¤ºï¼‰
    selected_workers = [0, NUM_WORKERS//2, NUM_WORKERS-1]
    for i, worker_id in enumerate(selected_workers):
        comm_pattern = communication_indicators[worker_id, :min(100, NUM_ROUNDS)]  # åªæ˜¾ç¤ºå‰100è½®
        ax4.plot(comm_pattern + i * 1.2, label=f'å®¢æˆ·ç«¯{worker_id}', linewidth=1.5)
    
    ax4.set_xlabel('è®­ç»ƒè½®æ¬¡')
    ax4.set_ylabel('é€šä¿¡æŒ‡ç¤ºå™¨')
    ax4.set_title('FLQæ‡’æƒ°èšåˆé€šä¿¡æ¨¡å¼')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨åˆ°resultsæ–‡ä»¶å¤¹
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    plot_file = f"flq_fed_{DATASET_TYPE}_{timestamp}.png"
    os.makedirs("results", exist_ok=True)
    plt.savefig(f"results/{plot_file}", dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"ğŸ“Š å®éªŒå›¾è¡¨å·²ä¿å­˜åˆ°: results/{plot_file}")


# ---------------------
# ä¸»ç¨‹åºå…¥å£
# ---------------------
if __name__ == "__main__":
    print("ğŸ¯ FLQè”é‚¦å­¦ä¹ å®ç°")
    print("åŸºäºlegacy/FLQ.pyçš„é‡åŒ–å’Œæ‡’æƒ°èšåˆæœºåˆ¶")
    print("æ”¯æŒMNISTå’ŒFashion-MNISTæ•°æ®é›†\n")
    
    # é€‰æ‹©æ•°æ®é›†
    # dataset_choice = input(f"é€‰æ‹©æ•°æ®é›† (mnist/fashion_mnist) [{DATASET_TYPE}]: ").strip() or DATASET_TYPE
    DATASET_TYPE = "mnist"
    
    results, server = run_flq_federated_learning()
    
    print(f"\nğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆï¼")
    print("ğŸ“ è¯·æŸ¥çœ‹results/ç›®å½•ä¸‹çš„ç»“æœæ–‡ä»¶å’Œå›¾è¡¨")

