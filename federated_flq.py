#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆFLQè”é‚¦å­¦ä¹ æµ‹è¯•è„šæœ¬
åŸºäºŽmaster.pyå’Œworker.pyï¼Œæ•´åˆä¸ºå•æ–‡ä»¶è¿è¡Œ
ä½¿ç”¨çœŸå®žMNISTæ•°æ®é›†æµ‹è¯•FLQé‡åŒ–ç®—æ³•æ•ˆæžœ
"""

import numpy as np
import time
import random
import json
import os
from typing import List, Tuple, Dict, Any
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from torchvision import datasets, transforms

# ---------------------
# å…¨å±€é…ç½®å‚æ•°
# ---------------------
NUM_WORKERS = 10       # è”é‚¦å­¦ä¹ å®¢æˆ·ç«¯æ•°é‡ï¼ˆå‡å°‘åŠ é€Ÿæµ‹è¯•ï¼‰
NUM_ROUNDS = 200        # è”é‚¦å­¦ä¹ è½®æ¬¡ï¼ˆå‡å°‘åŠ é€Ÿæµ‹è¯•ï¼‰
LOCAL_EPOCHS = 5      # æ¯ä¸ªå®¢æˆ·ç«¯çš„æœ¬åœ°è®­ç»ƒè½®æ•°ï¼ˆå‡å°‘åŠ é€Ÿæµ‹è¯•ï¼‰
BATCH_SIZE = 64       # æœ¬åœ°è®­ç»ƒæ‰¹æ¬¡å¤§å°
LR = 0.01            # å­¦ä¹ çŽ‡
SEED = 42            # éšæœºç§å­

# FLQé‡åŒ–é…ç½®
FLQ_MODE = "sign1"    # é‡åŒ–æ¨¡å¼: "off", "sign1", "int8", "4bit"

# MNISTæ•°æ®é›†é…ç½®
TRAIN_SIZE = 60000    # MNISTè®­ç»ƒé›†å¤§å°
TEST_SIZE = 10000     # MNISTæµ‹è¯•é›†å¤§å°

print(f"ðŸ“Š FLQè”é‚¦å­¦ä¹ æµ‹è¯•é…ç½®:")
print(f"  å®¢æˆ·ç«¯æ•°: {NUM_WORKERS}")
print(f"  è®­ç»ƒè½®æ¬¡: {NUM_ROUNDS}")
print(f"  é‡åŒ–æ¨¡å¼: {FLQ_MODE}")
print(f"  æœ¬åœ°è½®æ•°: {LOCAL_EPOCHS}")
print(f"  æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")

# è®¾ç½®éšæœºç§å­
np.random.seed(SEED)
random.seed(SEED)
torch.manual_seed(SEED)
rng = np.random.default_rng(SEED)

# è®¾å¤‡é…ç½®
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"  ä½¿ç”¨è®¾å¤‡: {device}")

# ---------------------
# PyTorch MNISTæ¨¡åž‹å®šä¹‰
# ---------------------
class MNISTNet(nn.Module):
    """ç®€å•çš„MNISTåˆ†ç±»ç½‘ç»œ"""
    def __init__(self):
        super(MNISTNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)

def get_model_parameters_count(model):
    """è®¡ç®—æ¨¡åž‹å‚æ•°æ•°é‡"""
    return sum(p.numel() for p in model.parameters())

def model_to_vector(model):
    """å°†æ¨¡åž‹å‚æ•°è½¬æ¢ä¸ºä¸€ç»´å‘é‡"""
    vec = []
    for param in model.parameters():
        vec.append(param.data.cpu().numpy().flatten())
    return np.concatenate(vec)

def vector_to_model(vector, model):
    """å°†ä¸€ç»´å‘é‡è½¬æ¢å›žæ¨¡åž‹å‚æ•°"""
    start_idx = 0
    for param in model.parameters():
        param_length = param.numel()
        param_data = vector[start_idx:start_idx + param_length]
        param.data = torch.from_numpy(param_data.reshape(param.shape)).float().to(param.device)
        start_idx += param_length

# ---------------------
# MNISTæ•°æ®é›†åŠ è½½å’Œåˆ†å‰²
# ---------------------
def load_mnist_data():
    """åŠ è½½MNISTæ•°æ®é›†"""
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)
    
    return train_dataset, test_dataset

def split_dataset_for_workers_iid(dataset, num_workers):
    """
    ä¸ºå¤šä¸ªworkeråˆ†å‰²æ•°æ®é›†ï¼ˆå®Œå…¨IIDæ•°æ®åˆ†å¸ƒï¼‰
    æ¯ä¸ªworkeréƒ½åŒ…å«æ‰€æœ‰ç±»åˆ«ï¼Œä¸”ç±»åˆ«åˆ†å¸ƒç›¸åŒ
    """
    num_items = len(dataset)
    
    # èŽ·å–æ ‡ç­¾
    if hasattr(dataset, 'targets'):
        labels = dataset.targets
    else:
        labels = [dataset[i][1] for i in range(len(dataset))]
    
    labels = np.array(labels)
    num_classes = len(np.unique(labels))
    
    # æŒ‰ç±»åˆ«ç»„ç»‡æ•°æ®ç´¢å¼•
    class_indices = {}
    for cls in range(num_classes):
        class_indices[cls] = np.where(labels == cls)[0]
    
    worker_datasets = []
    
    print("ðŸ“Š ä½¿ç”¨IIDï¼ˆç‹¬ç«‹åŒåˆ†å¸ƒï¼‰æ•°æ®åˆ†å‰²æ–¹å¼")
    
    for worker_id in range(num_workers):
        worker_indices = []
        
        # æ¯ä¸ªworkerä»Žæ¯ä¸ªç±»åˆ«ä¸­èŽ·å¾—ç›¸åŒæ¯”ä¾‹çš„æ•°æ®
        for cls in range(num_classes):
            cls_data = class_indices[cls]
            # å°†è¯¥ç±»åˆ«çš„æ•°æ®å¹³å‡åˆ†é…ç»™æ‰€æœ‰workers
            start_idx = worker_id * len(cls_data) // num_workers
            end_idx = (worker_id + 1) * len(cls_data) // num_workers
            worker_indices.extend(cls_data[start_idx:end_idx])
        
        # éšæœºæ‰“ä¹±ç´¢å¼•ï¼Œä¿è¯æ•°æ®çš„éšæœºæ€§
        np.random.shuffle(worker_indices)
        worker_datasets.append(Subset(dataset, worker_indices))
        
        # ç»Ÿè®¡æ¯ä¸ªworkerçš„ç±»åˆ«åˆ†å¸ƒ
        worker_labels = [labels[i] for i in worker_indices]
        unique, counts = np.unique(worker_labels, return_counts=True)
        print(f"Worker {worker_id} IIDåˆ†å¸ƒ ({len(worker_indices)}æ ·æœ¬): {dict(zip(unique, counts))}")
    
    return worker_datasets

def split_dataset_for_workers(dataset, num_workers, alpha=0.5):
    """
    ä¸ºå¤šä¸ªworkeråˆ†å‰²æ•°æ®é›†ï¼ˆæ¨¡æ‹ŸNon-IIDæ•°æ®åˆ†å¸ƒï¼‰
    alpha: æŽ§åˆ¶æ•°æ®åˆ†å¸ƒçš„ä¸å‡åŒ€ç¨‹åº¦ï¼Œ0=å®Œå…¨Non-IIDï¼Œ1=å®Œå…¨IID
    """
    num_items = len(dataset)
    items_per_worker = num_items // num_workers
    
    # èŽ·å–æ ‡ç­¾
    if hasattr(dataset, 'targets'):
        labels = dataset.targets
    else:
        labels = [dataset[i][1] for i in range(len(dataset))]
    
    labels = np.array(labels)
    num_classes = len(np.unique(labels))
    
    worker_datasets = []
    
    for worker_id in range(num_workers):
        if alpha == 1.0:
            # å®Œå…¨IIDï¼šéšæœºåˆ†é…
            indices = np.random.choice(num_items, items_per_worker, replace=False)
        else:
            # Non-IIDï¼šæ¯ä¸ªworkerä¸»è¦åŒ…å«ç‰¹å®šç±»åˆ«çš„æ•°æ®
            primary_classes = np.random.choice(num_classes, max(1, int(alpha * num_classes)), replace=False)
            
            # èŽ·å–ä¸»è¦ç±»åˆ«çš„ç´¢å¼•
            primary_indices = []
            for cls in primary_classes:
                cls_indices = np.where(labels == cls)[0]
                primary_indices.extend(cls_indices[:items_per_worker // len(primary_classes)])
            
            # éšæœºæ·»åŠ å…¶ä»–ç±»åˆ«çš„å°‘é‡æ•°æ®
            remaining_count = items_per_worker - len(primary_indices)
            if remaining_count > 0:
                other_indices = np.setdiff1d(range(num_items), primary_indices)
                additional_indices = np.random.choice(other_indices, remaining_count, replace=False)
                primary_indices.extend(additional_indices)
            
            indices = primary_indices[:items_per_worker]
        
        worker_dataset = Subset(dataset, indices)
        worker_datasets.append(worker_dataset)
        
        # ç»Ÿè®¡æ¯ä¸ªworkerçš„ç±»åˆ«åˆ†å¸ƒ
        worker_labels = [labels[i] for i in indices]
        unique, counts = np.unique(worker_labels, return_counts=True)
        print(f"Worker {worker_id} æ•°æ®åˆ†å¸ƒ: {dict(zip(unique, counts))}")
    
    return worker_datasets

# ---------------------
# FLQé‡åŒ–ç®—æ³•ï¼ˆä»Žflq_quantization.pyç®€åŒ–ï¼‰
# ---------------------
def flq_sign_quantization(gradients):
    """FLQç¬¦å·é‡åŒ–ï¼ˆ1ä½ï¼‰"""
    signs = np.sign(gradients)
    scale_factor = np.mean(np.abs(gradients))
    quantized = signs * scale_factor
    logical_bits = len(gradients)  # æ¯å‚æ•°1ä½
    return quantized, scale_factor, logical_bits

def flq_8bit_quantization(gradients):
    """FLQ 8ä½é‡åŒ–"""
    max_abs = np.max(np.abs(gradients))
    if max_abs == 0:
        return gradients, 1.0, 8 * len(gradients)
    
    scale_factor = max_abs / 127.0
    quantized_int8 = np.clip(np.round(gradients / scale_factor), -128, 127)
    quantized = quantized_int8 * scale_factor
    logical_bits = 8 * len(gradients)
    return quantized, scale_factor, logical_bits

def flq_4bit_quantization(gradients):
    """FLQ 4ä½é‡åŒ–"""
    max_abs = np.max(np.abs(gradients))
    if max_abs == 0:
        return gradients, 1.0, 4 * len(gradients)
    
    scale_factor = max_abs / 7.0
    quantized_int4 = np.clip(np.round(gradients / scale_factor), -8, 7)
    quantized = quantized_int4 * scale_factor
    logical_bits = 4 * len(gradients)
    return quantized, scale_factor, logical_bits

def apply_flq_quantization(gradients, mode):
    """åº”ç”¨FLQé‡åŒ–"""
    if mode == "off":
        return gradients, 32 * len(gradients)
    elif mode == "sign1":
        quantized, scale, logical_bits = flq_sign_quantization(gradients)
        return quantized, logical_bits
    elif mode == "int8":
        quantized, scale, logical_bits = flq_8bit_quantization(gradients)
        return quantized, logical_bits
    elif mode == "4bit":
        quantized, scale, logical_bits = flq_4bit_quantization(gradients)
        return quantized, logical_bits
    else:
        raise ValueError(f"Unknown FLQ mode: {mode}")

# ---------------------
# å‚æ•°æœåŠ¡å™¨ç±»ï¼ˆåŸºäºŽmaster.pyï¼‰
# ---------------------
class FederatedMaster:
    """è”é‚¦å­¦ä¹ å‚æ•°æœåŠ¡å™¨"""
    
    def __init__(self, min_clients: int = 10):
        # åˆ›å»ºå…¨å±€æ¨¡åž‹æ¥èŽ·å–å‚æ•°ç»´åº¦
        global_model = MNISTNet()
        self.model_dim = get_model_parameters_count(global_model)
        self.global_weights = model_to_vector(global_model)
        
        self.min_clients = min_clients
        self.global_round = 0
        self.pending_updates = {}
        self.workers_status = {}
        
        print(f"ðŸ›ï¸ å‚æ•°æœåŠ¡å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"  æ¨¡åž‹å‚æ•°é‡: {self.model_dim}")
        print(f"  æœ€å°å®¢æˆ·ç«¯æ•°: {min_clients}")
    
    def get_global_model(self) -> Tuple[np.ndarray, int]:
        """èŽ·å–å…¨å±€æ¨¡åž‹"""
        return self.global_weights.copy(), self.global_round
    
    def receive_update(self, worker_id: str, round_id: int, delta: np.ndarray, 
                      num_samples: int, loss: float) -> Dict[str, Any]:
        """æŽ¥æ”¶å®¢æˆ·ç«¯æ›´æ–°"""
        # éªŒè¯ç»´åº¦
        if delta.shape[0] != self.model_dim:
            return {"status": "error", "msg": f"Dimension mismatch: {delta.shape[0]} vs {self.model_dim}"}
        
        # è®°å½•workerçŠ¶æ€
        self.workers_status[worker_id] = {
            "round": round_id,
            "num_samples": num_samples,
            "loss": loss,
            "timestamp": time.time()
        }
        
        # ç´¯ç§¯æ›´æ–°
        if round_id not in self.pending_updates:
            self.pending_updates[round_id] = []
        
        self.pending_updates[round_id].append({
            "worker_id": worker_id,
            "delta": delta,
            "num_samples": num_samples,
            "loss": loss
        })
        
        # æ£€æŸ¥æ˜¯å¦å¯ä»¥èšåˆ
        aggregated = False
        if (round_id == self.global_round and 
            len(self.pending_updates[round_id]) >= self.min_clients):
            aggregated = self._aggregate_updates(round_id)
        
        return {
            "status": "ok",
            "aggregated": aggregated,
            "global_round": self.global_round
        }
    
    def _aggregate_updates(self, round_id: int) -> bool:
        """FedAvgèšåˆç®—æ³•"""
        updates = self.pending_updates[round_id]
        
        # è®¡ç®—æ€»æ ·æœ¬æ•°
        total_samples = sum(u["num_samples"] for u in updates)
        
        if total_samples > 0:
            # åŠ æƒå¹³å‡èšåˆ
            agg_delta = np.zeros_like(self.global_weights)
            for update in updates:
                weight = update["num_samples"] / total_samples
                agg_delta += weight * update["delta"]
        else:
            # ç®€å•å¹³å‡
            agg_delta = np.mean([u["delta"] for u in updates], axis=0)
        
        # æ›´æ–°å…¨å±€æ¨¡åž‹
        self.global_weights = self.global_weights + agg_delta
        self.global_round += 1
        
        # æ¸…ç†å·²èšåˆçš„æ›´æ–°
        del self.pending_updates[round_id]
        
        print(f"ðŸ”„ ç¬¬{round_id}è½®èšåˆå®Œæˆï¼Œå‚ä¸Žå®¢æˆ·ç«¯: {len(updates)}")
        avg_loss = np.mean([u["loss"] for u in updates])
        print(f"  å¹³å‡æŸå¤±: {avg_loss:.4f}")
        
        return True

# ---------------------
# è”é‚¦å­¦ä¹ å®¢æˆ·ç«¯ç±»ï¼ˆåŸºäºŽworker.pyï¼‰
# ---------------------
class FederatedWorker:
    """è”é‚¦å­¦ä¹ å®¢æˆ·ç«¯"""
    
    def __init__(self, worker_id: str, dataset, test_dataset):
        self.worker_id = worker_id
        self.dataset = dataset
        self.test_dataset = test_dataset
        self.dataset_size = len(dataset)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.train_loader = DataLoader(
            dataset, 
            batch_size=BATCH_SIZE, 
            shuffle=True,
            drop_last=True
        )
        
        # åˆ›å»ºæœ¬åœ°æ¨¡åž‹
        self.model = MNISTNet().to(device)
        self.criterion = nn.NLLLoss()
        
        print(f"ðŸ‘¤ å®¢æˆ·ç«¯ {worker_id} åˆå§‹åŒ–å®Œæˆ")
        print(f"  æœ¬åœ°æ•°æ®é‡: {self.dataset_size}")
        print(f"  æ¨¡åž‹å‚æ•°é‡: {get_model_parameters_count(self.model)}")
    
    def local_training(self, global_weights: np.ndarray, epochs: int, lr: float) -> Tuple[np.ndarray, float, float]:
        """çœŸå®žçš„æœ¬åœ°MNISTè®­ç»ƒ"""
        # 1. è®¾ç½®å…¨å±€æƒé‡åˆ°æœ¬åœ°æ¨¡åž‹
        vector_to_model(global_weights, self.model)
        initial_weights = global_weights.copy()
        
        # 2. åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = optim.SGD(self.model.parameters(), lr=lr, momentum=0.9)
        
        # 3. æœ¬åœ°è®­ç»ƒ
        self.model.train()
        total_loss = 0.0
        total_samples = 0
        correct = 0
        
        for epoch in range(epochs):
            epoch_loss = 0.0
            epoch_samples = 0
            
            for batch_idx, (data, target) in enumerate(self.train_loader):
                data, target = data.to(device), target.to(device)
                
                optimizer.zero_grad()
                output = self.model(data)
                loss = self.criterion(output, target)
                loss.backward()
                optimizer.step()
                
                # ç»Ÿè®¡
                epoch_loss += loss.item()
                epoch_samples += len(data)
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
            
            total_loss += epoch_loss
            total_samples += epoch_samples
            
            if epoch == 0:  # åªåœ¨ç¬¬ä¸€ä¸ªepochæ‰“å°è¯¦ç»†ä¿¡æ¯
                print(f"  [{self.worker_id}] Epoch {epoch+1}/{epochs}: "
                      f"Loss={epoch_loss/len(self.train_loader):.4f}")
        
        # 4. è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        avg_loss = total_loss / (epochs * len(self.train_loader))
        accuracy = correct / total_samples if total_samples > 0 else 0.0
        
        # 5. è®¡ç®—æƒé‡æ›´æ–°
        final_weights = model_to_vector(self.model)
        delta = final_weights - initial_weights
        
        return delta, avg_loss, accuracy
    
    def evaluate_model(self, global_weights: np.ndarray = None) -> Tuple[float, float]:
        """è¯„ä¼°æ¨¡åž‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½"""
        if global_weights is not None:
            vector_to_model(global_weights, self.model)
        
        self.model.eval()
        test_loss = 0
        correct = 0
        total = 0
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨ï¼ˆåªä½¿ç”¨å‰1000ä¸ªæ ·æœ¬åŠ é€Ÿæµ‹è¯•ï¼‰
        test_subset = Subset(self.test_dataset, range(min(1000, len(self.test_dataset))))
        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False)
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                output = self.model(data)
                test_loss += self.criterion(output, target).item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += len(data)
        
        avg_loss = test_loss / len(test_loader)
        accuracy = correct / total if total > 0 else 0.0
        
        return avg_loss, accuracy
    
    def participate_round(self, master: FederatedMaster, flq_mode: str) -> Dict[str, Any]:
        """å‚ä¸Žä¸€è½®è”é‚¦å­¦ä¹ """
        # 1. ä»ŽæœåŠ¡å™¨èŽ·å–å…¨å±€æ¨¡åž‹
        global_weights, current_round = master.get_global_model()
        
        # 2. æœ¬åœ°è®­ç»ƒ
        delta, train_loss, train_accuracy = self.local_training(global_weights, LOCAL_EPOCHS, LR)
        
        # 3. FLQé‡åŒ–
        quantized_delta, logical_bits = apply_flq_quantization(delta, flq_mode)
        
        # 4. è®¡ç®—é€šä¿¡å¼€é”€
        original_bits = 32 * len(delta)  # åŽŸå§‹32ä½æµ®ç‚¹æ•°
        compression_ratio = original_bits / logical_bits if logical_bits > 0 else 1.0
        
        # 5. ä½¿ç”¨å®žé™…å‚ä¸Žçš„æ ·æœ¬æ•°é‡
        num_samples = self.dataset_size
        
        # 6. ä¸ŠæŠ¥ç»™æœåŠ¡å™¨
        resp = master.receive_update(
            worker_id=self.worker_id,
            round_id=current_round,
            delta=quantized_delta,
            num_samples=num_samples,
            loss=train_loss
        )
        
        return {
            "round": current_round,
            "train_loss": train_loss,
            "train_accuracy": train_accuracy,
            "num_samples": num_samples,
            "logical_bits": logical_bits,
            "compression_ratio": compression_ratio,
            "response": resp
        }

# ---------------------
# å®žéªŒç»“æžœè®°å½•å’Œåˆ†æž
# ---------------------
class ExperimentLogger:
    """å®žéªŒæ—¥å¿—è®°å½•å™¨ - è®°å½•è®­ç»ƒç»“æžœåˆ°JSONæ–‡ä»¶ç”¨äºŽç»˜å›¾åˆ†æž"""
    
    def __init__(self, experiment_name: str = "flq_experiment"):
        self.experiment_name = experiment_name
        self.results = []
        self.communication_costs = []
        self.accuracies = []
        
        # æ ¹æ®è®ºæ–‡å›¾ç‰‡è®¾è®¡çš„æ•°æ®ç»“æž„
        self.experiment_data = {
            "experiment_info": {
                "name": experiment_name,
                "timestamp": time.strftime("%Y-%m-%d_%H-%M-%S"),
                "config": {
                    "num_workers": NUM_WORKERS,
                    "num_rounds": NUM_ROUNDS,
                    "local_epochs": LOCAL_EPOCHS,
                    "batch_size": BATCH_SIZE,
                    "learning_rate": LR,
                    "flq_mode": FLQ_MODE
                }
            },
            "convergence_data": {
                "rounds": [],
                "global_test_loss": [],
                "global_test_accuracy": [],
                "average_train_loss": [],
                "average_train_accuracy": [],
                "communication_bits": [],
                "compression_ratios": []
            },
            "communication_data": {
                "total_upload_bits": 0,
                "total_broadcast_bits": 0,
                "per_round_bits": [],
                "quantization_levels": []
            },
            "quantization_analysis": {
                "mode": FLQ_MODE,
                "rmse_values": [],
                "approximation_quality": [],
                "binary_values": []
            },
            "resource_optimization": {
                "final_accuracy": 0.0,
                "total_iterations": 0,
                "total_communication_bits": 0,
                "convergence_round": -1
            }
        }
    
    def log_round(self, round_id: int, worker_results: List[Dict], global_loss: float, global_accuracy: float = 0.0):
        """è®°å½•ä¸€è½®å®žéªŒç»“æžœ"""
        # èšåˆæœ¬è½®ç»Ÿè®¡
        total_bits = sum(r["logical_bits"] for r in worker_results)
        avg_compression = np.mean([r["compression_ratio"] for r in worker_results])
        avg_train_loss = np.mean([r["train_loss"] for r in worker_results])
        avg_train_accuracy = np.mean([r["train_accuracy"] for r in worker_results])
        
        round_result = {
            "round": round_id,
            "global_loss": global_loss,
            "global_accuracy": global_accuracy,
            "avg_train_loss": avg_train_loss,
            "avg_train_accuracy": avg_train_accuracy,
            "total_communication_bits": total_bits,
            "avg_compression_ratio": avg_compression,
            "num_workers": len(worker_results)
        }
        
        self.results.append(round_result)
        self.communication_costs.append(total_bits)
        self.accuracies.append(global_accuracy)
        
        # æ›´æ–°JSONæ•°æ®ç»“æž„ - ç”¨äºŽè®ºæ–‡å›¾è¡¨ç»˜åˆ¶
        self._update_json_data(round_id, worker_results, global_loss, global_accuracy, 
                              total_bits, avg_compression, avg_train_loss, avg_train_accuracy)
        
        print(f"ðŸ“ˆ ç¬¬{round_id}è½®ç»“æžœ:")
        print(f"  å…¨å±€æµ‹è¯•æŸå¤±: {global_loss:.4f}")
        print(f"  å…¨å±€æµ‹è¯•ç²¾åº¦: {global_accuracy:.3f}")
        print(f"  å¹³å‡è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
        print(f"  å¹³å‡è®­ç»ƒç²¾åº¦: {avg_train_accuracy:.3f}")
        print(f"  é€šä¿¡å¼€é”€: {total_bits:,} bits")
        print(f"  å¹³å‡åŽ‹ç¼©æ¯”: {avg_compression:.1f}:1")
    
    def _update_json_data(self, round_id: int, worker_results: List[Dict], global_loss: float, 
                         global_accuracy: float, total_bits: int, avg_compression: float,
                         avg_train_loss: float, avg_train_accuracy: float):
        """æ›´æ–°JSONæ•°æ®ç»“æž„"""
        # æ”¶æ•›æ•°æ® (å¯¹åº”è®ºæ–‡å›¾2)
        conv_data = self.experiment_data["convergence_data"]
        conv_data["rounds"].append(round_id)
        conv_data["global_test_loss"].append(float(global_loss))
        conv_data["global_test_accuracy"].append(float(global_accuracy))
        conv_data["average_train_loss"].append(float(avg_train_loss))
        conv_data["average_train_accuracy"].append(float(avg_train_accuracy))
        conv_data["communication_bits"].append(int(total_bits))
        conv_data["compression_ratios"].append(float(avg_compression))
        
        # é€šä¿¡æ•°æ® (å¯¹åº”è®ºæ–‡å›¾3å’Œè¡¨1)
        comm_data = self.experiment_data["communication_data"]
        comm_data["per_round_bits"].append(int(total_bits))
        comm_data["total_upload_bits"] += total_bits  # å®¢æˆ·ç«¯ä¸Šä¼ 
        comm_data["total_broadcast_bits"] += total_bits // len(worker_results)  # æœåŠ¡å™¨å¹¿æ’­
        
        # é‡åŒ–åˆ†æž (å¯¹åº”è®ºæ–‡å›¾4)
        quant_data = self.experiment_data["quantization_analysis"]
        if FLQ_MODE == "sign1":
            # è®¡ç®—äºŒè¿›åˆ¶å€¼ (0æˆ–1)
            binary_vals = [1.0 if r["compression_ratio"] > 1.0 else 0.0 for r in worker_results]
            quant_data["binary_values"].extend(binary_vals)
        
        # è®¡ç®—RMSE (è¿‘ä¼¼è¯¯å·®)
        if len(worker_results) > 0:
            # ç®€åŒ–çš„RMSEè®¡ç®—ï¼šåŸºäºŽåŽ‹ç¼©æ¯”çš„è¿‘ä¼¼è¯¯å·®
            rmse = 1.0 / avg_compression if avg_compression > 0 else 1.0
            quant_data["rmse_values"].append(float(rmse))
            quant_data["approximation_quality"].append(float(global_accuracy))
        
        # èµ„æºä¼˜åŒ–æ±‡æ€» (å¯¹åº”è®ºæ–‡è¡¨1)
        resource_data = self.experiment_data["resource_optimization"]
        resource_data["final_accuracy"] = float(global_accuracy)
        resource_data["total_iterations"] = round_id + 1
        resource_data["total_communication_bits"] = comm_data["total_upload_bits"]
        
        # æ£€æµ‹æ”¶æ•›ç‚¹ï¼ˆæŸå¤±å˜åŒ–å°äºŽé˜ˆå€¼ï¼‰
        if (len(conv_data["global_test_loss"]) > 5 and 
            resource_data["convergence_round"] == -1):
            recent_losses = conv_data["global_test_loss"][-5:]
            if max(recent_losses) - min(recent_losses) < 0.01:  # æ”¶æ•›é˜ˆå€¼
                resource_data["convergence_round"] = round_id
    
    def plot_results(self, flq_mode: str):
        """ç»˜åˆ¶å®žéªŒç»“æžœ"""
        if not self.results:
            print("âš ï¸ æ²¡æœ‰å®žéªŒæ•°æ®å¯ç»˜åˆ¶")
            return
        
        rounds = [r["round"] for r in self.results]
        global_losses = [r["global_loss"] for r in self.results]
        global_accuracies = [r["global_accuracy"] for r in self.results]
        comm_costs = [r["total_communication_bits"] / 1e6 for r in self.results]  # è½¬æ¢ä¸ºMB
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # æŸå¤±æ›²çº¿
        ax1.plot(rounds, global_losses, 'b-o', linewidth=2, markersize=4)
        ax1.set_xlabel('è”é‚¦å­¦ä¹ è½®æ¬¡')
        ax1.set_ylabel('å…¨å±€æµ‹è¯•æŸå¤±')
        ax1.set_title(f'FLQ-{flq_mode} æŸå¤±æ”¶æ•›æ›²çº¿')
        ax1.grid(True, alpha=0.3)
        
        # å‡†ç¡®çŽ‡æ›²çº¿
        ax2.plot(rounds, global_accuracies, 'g-o', linewidth=2, markersize=4)
        ax2.set_xlabel('è”é‚¦å­¦ä¹ è½®æ¬¡')
        ax2.set_ylabel('å…¨å±€æµ‹è¯•å‡†ç¡®çŽ‡')
        ax2.set_title(f'FLQ-{flq_mode} å‡†ç¡®çŽ‡æ›²çº¿')
        ax2.grid(True, alpha=0.3)
        ax2.set_ylim(0, 1)
        
        # é€šä¿¡å¼€é”€
        ax3.plot(rounds, comm_costs, 'r-s', linewidth=2, markersize=4)
        ax3.set_xlabel('è”é‚¦å­¦ä¹ è½®æ¬¡')
        ax3.set_ylabel('é€šä¿¡å¼€é”€ (Mbits)')
        ax3.set_title(f'FLQ-{flq_mode} é€šä¿¡å¼€é”€')
        ax3.grid(True, alpha=0.3)
        
        # å‡†ç¡®çŽ‡-é€šä¿¡å¼€é”€æ•ˆçŽ‡å›¾
        ax4.plot(comm_costs, global_accuracies, 'm-d', linewidth=2, markersize=4)
        ax4.set_xlabel('ç´¯è®¡é€šä¿¡å¼€é”€ (Mbits)')
        ax4.set_ylabel('å…¨å±€æµ‹è¯•å‡†ç¡®çŽ‡')
        ax4.set_title(f'FLQ-{flq_mode} é€šä¿¡æ•ˆçŽ‡')
        ax4.grid(True, alpha=0.3)
        ax4.set_ylim(0, 1)
        
        plt.tight_layout()
        plt.savefig(f'flq_{flq_mode}_results.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"ðŸ“Š å®žéªŒç»“æžœå›¾å·²ä¿å­˜ä¸º flq_{flq_mode}_results.png")
    
    def save_experiment_data(self, filename: str = None):
        """ä¿å­˜å®žéªŒæ•°æ®åˆ°JSONæ–‡ä»¶"""
        if filename is None:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            flq_mode = self.experiment_data["experiment_info"]["config"]["flq_mode"]
            filename = f"experiment_results_{flq_mode}_{timestamp}.json"
        
        # ç¡®ä¿resultsç›®å½•å­˜åœ¨
        os.makedirs("results", exist_ok=True)
        filepath = os.path.join("results", filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(self.experiment_data, f, indent=2, ensure_ascii=False)
            print(f"ðŸ“„ å®žéªŒæ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
            return filepath
        except Exception as e:
            print(f"âŒ ä¿å­˜å®žéªŒæ•°æ®å¤±è´¥: {e}")
            return None
    
    def load_experiment_data(self, filepath: str):
        """ä»ŽJSONæ–‡ä»¶åŠ è½½å®žéªŒæ•°æ®"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                self.experiment_data = json.load(f)
            print(f"ðŸ“„ å®žéªŒæ•°æ®å·²ä»Ž {filepath} åŠ è½½")
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½å®žéªŒæ•°æ®å¤±è´¥: {e}")
            return False
    
    def get_summary_for_comparison(self):
        """èŽ·å–ç”¨äºŽå¯¹æ¯”çš„å®žéªŒæ‘˜è¦æ•°æ®"""
        resource_data = self.experiment_data["resource_optimization"]
        conv_data = self.experiment_data["convergence_data"]
        comm_data = self.experiment_data["communication_data"]
        
        return {
            "mode": self.experiment_data["quantization_analysis"]["mode"],
            "final_accuracy": resource_data["final_accuracy"],
            "final_loss": conv_data["global_test_loss"][-1] if conv_data["global_test_loss"] else 0.0,
            "total_communication_bits": resource_data["total_communication_bits"],
            "convergence_round": resource_data["convergence_round"],
            "total_iterations": resource_data["total_iterations"],
            "avg_compression_ratio": np.mean(conv_data["compression_ratios"]) if conv_data["compression_ratios"] else 1.0
        }

# ---------------------
# ä¸»å®žéªŒå‡½æ•°
# ---------------------
def run_federated_flq_experiment(flq_mode: str = "sign1"):
    """è¿è¡ŒFLQè”é‚¦å­¦ä¹ å®žéªŒ"""
    print(f"\nðŸš€ å¼€å§‹FLQ-{flq_mode}è”é‚¦å­¦ä¹ å®žéªŒ")
    print("=" * 50)
    
    # 1. åŠ è½½å’Œåˆ†å‰²MNISTæ•°æ®é›†
    print("ðŸ“¥ åŠ è½½MNISTæ•°æ®é›†...")
    train_dataset, test_dataset = load_mnist_data()
    worker_datasets = split_dataset_for_workers(train_dataset, NUM_WORKERS, alpha=0.5)
    
    # 2. åˆå§‹åŒ–å‚æ•°æœåŠ¡å™¨
    master = FederatedMaster(min_clients=10)
    
    # 3. åˆå§‹åŒ–å®¢æˆ·ç«¯
    workers = []
    for i in range(NUM_WORKERS):
        worker = FederatedWorker(f"worker_{i}", worker_datasets[i], test_dataset)
        workers.append(worker)
    
    # 4. åˆå§‹åŒ–å®žéªŒè®°å½•å™¨
    experiment_name = f"flq_{flq_mode}_federated_learning"
    logger = ExperimentLogger(experiment_name)
    
    # 5. å¼€å§‹è”é‚¦å­¦ä¹ è®­ç»ƒ
    for round_id in range(NUM_ROUNDS):
        print(f"\nðŸ”„ ç¬¬{round_id}è½®è”é‚¦å­¦ä¹ å¼€å§‹...")
        
        # æ‰€æœ‰å®¢æˆ·ç«¯å‚ä¸Žè®­ç»ƒ
        worker_results = []
        for worker in workers:
            result = worker.participate_round(master, flq_mode)
            worker_results.append(result)
            
            print(f"  {worker.worker_id}: train_loss={result['train_loss']:.4f}, "
                  f"train_acc={result['train_accuracy']:.3f}, "
                  f"compression={result['compression_ratio']:.1f}:1")
        
        # è¯„ä¼°å…¨å±€æ¨¡åž‹æ€§èƒ½
        global_weights, _ = master.get_global_model()
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªworkeræ¥è¯„ä¼°å…¨å±€æ¨¡åž‹ï¼ˆæ‰€æœ‰workerå…±äº«æµ‹è¯•é›†ï¼‰
        test_loss, test_accuracy = workers[0].evaluate_model(global_weights)
        
        print(f"ðŸŽ¯ å…¨å±€æ¨¡åž‹è¯„ä¼°: test_loss={test_loss:.4f}, test_acc={test_accuracy:.3f}")
        
        # è®°å½•å®žéªŒç»“æžœ
        logger.log_round(round_id, worker_results, test_loss, test_accuracy)
        
        # çŸ­æš‚é—´éš”
        time.sleep(0.1)
    
    print(f"\nâœ… FLQ-{flq_mode}è”é‚¦å­¦ä¹ å®žéªŒå®Œæˆï¼")
    
    # 6. ç”Ÿæˆå®žéªŒæŠ¥å‘Š
    final_loss = logger.results[-1]["global_loss"]
    final_accuracy = logger.results[-1]["global_accuracy"]
    total_comm = sum(logger.communication_costs) / 1e6  # è½¬æ¢ä¸ºMbits
    avg_compression = np.mean([r["avg_compression_ratio"] for r in logger.results])
    
    print(f"\nðŸ“Š å®žéªŒæ€»ç»“:")
    print(f"  æœ€ç»ˆå…¨å±€æŸå¤±: {final_loss:.4f}")
    print(f"  æœ€ç»ˆå…¨å±€å‡†ç¡®çŽ‡: {final_accuracy:.3f}")
    print(f"  æ€»é€šä¿¡å¼€é”€: {total_comm:.2f} Mbits")
    print(f"  å¹³å‡åŽ‹ç¼©æ¯”: {avg_compression:.1f}:1")
    
    # 6. ä¿å­˜å®žéªŒæ•°æ®åˆ°JSONæ–‡ä»¶
    json_filepath = logger.save_experiment_data()
    if json_filepath:
        print(f"âœ… è®­ç»ƒæ•°æ®å·²ä¿å­˜ï¼Œå¯ç”¨äºŽç»˜åˆ¶è®ºæ–‡å›¾è¡¨")
    
    # 7. ç»˜åˆ¶è®ºæ–‡å›¾è¡¨
    try:
        from plot_experiment import PlotExperiment
        plotter = PlotExperiment()
        # åŠ è½½åˆšä¿å­˜çš„å®žéªŒæ•°æ®
        if json_filepath:
            json_filename = os.path.basename(json_filepath)
            plotter.load_experiment_data(json_filename)
            # ç»˜åˆ¶å•ä¸ªå®žéªŒçš„å›¾è¡¨
            plotter.plot_gradient_quantization(flq_mode)
            print("âœ… è®ºæ–‡å›¾è¡¨å·²ç”Ÿæˆ")
    except Exception as e:
        print(f"âš ï¸ ç»˜å›¾åŠŸèƒ½å¯é€‰ï¼Œè·³è¿‡: {e}")
    
    # 8. ä¼ ç»Ÿç»˜åˆ¶ç»“æžœï¼ˆå¯é€‰ï¼‰
    # logger.plot_results(flq_mode)
    
    return logger

# ---------------------
# å¯¹æ¯”å®žéªŒå‡½æ•°
# ---------------------
def compare_flq_modes():
    """å¯¹æ¯”ä¸åŒFLQé‡åŒ–æ¨¡å¼"""
    modes = ["off", "sign1", "int8", "4bit"]
    results = {}
    
    print(f"\nðŸ”¬ å¼€å§‹FLQé‡åŒ–æ¨¡å¼å¯¹æ¯”å®žéªŒ")
    print("=" * 60)
    
    for mode in modes:
        print(f"\n{'='*20} FLQ-{mode} {'='*20}")
        logger = run_federated_flq_experiment(mode)
        results[mode] = logger
    
    # æ±‡æ€»å¯¹æ¯”ç»“æžœ
    print(f"\nðŸ“ˆ FLQé‡åŒ–æ¨¡å¼å¯¹æ¯”æ€»ç»“:")
    print("-" * 60)
    print(f"{'æ¨¡å¼':<10} {'æœ€ç»ˆæŸå¤±':<12} {'æœ€ç»ˆç²¾åº¦':<12} {'æ€»é€šä¿¡(Mbits)':<15} {'å¹³å‡åŽ‹ç¼©æ¯”':<12}")
    print("-" * 60)
    
    comparison_data = {
        "comparison_timestamp": time.strftime("%Y-%m-%d_%H-%M-%S"),
        "modes_compared": list(modes),
        "summary": []
    }
    
    for mode, logger in results.items():
        summary = logger.get_summary_for_comparison()
        comparison_data["summary"].append(summary)
        
        print(f"{mode:<10} {summary['final_loss']:<12.4f} {summary['final_accuracy']:<12.3f} "
              f"{summary['total_communication_bits']/1e6:<15.2f} {summary['avg_compression_ratio']:<12.1f}")
    
    # ä¿å­˜å¯¹æ¯”ç»“æžœåˆ°JSON
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    comparison_filepath = os.path.join("results", f"flq_modes_comparison_{timestamp}.json")
    os.makedirs("results", exist_ok=True)
    
    try:
        with open(comparison_filepath, 'w', encoding='utf-8') as f:
            json.dump(comparison_data, f, indent=2, ensure_ascii=False)
        print(f"\nðŸ“„ å¯¹æ¯”ç»“æžœå·²ä¿å­˜åˆ°: {comparison_filepath}")
    except Exception as e:
        print(f"âŒ ä¿å­˜å¯¹æ¯”ç»“æžœå¤±è´¥: {e}")
    
    # ç”Ÿæˆå®Œæ•´çš„å¯¹æ¯”å›¾è¡¨
    try:
        from plot_experiment import PlotExperiment
        plotter = PlotExperiment()
        plotter.load_all_experiments()
        plotter.load_comparison_data()
        plotter.plot_all_figures()
        print("âœ… å®Œæ•´çš„è®ºæ–‡å¯¹æ¯”å›¾è¡¨å·²ç”Ÿæˆ")
    except Exception as e:
        print(f"âš ï¸ å¯¹æ¯”å›¾è¡¨ç”Ÿæˆå¯é€‰ï¼Œè·³è¿‡: {e}")
    
    return results

# ---------------------
# ä¸»ç¨‹åºå…¥å£
# ---------------------
if __name__ == "__main__":
    print("ðŸŽ¯ FLQè”é‚¦å­¦ä¹ ç®€åŒ–æµ‹è¯•è„šæœ¬")
    print("åŸºäºŽè®ºæ–‡ã€ŠFederated Optimal Framework with Low-bitwidth Quantizationã€‹")
    print("æ•´åˆmaster.pyå’Œworker.pyåŠŸèƒ½ï¼Œå•æ–‡ä»¶è¿è¡Œ")
    
    # é€‰æ‹©å®žéªŒæ¨¡å¼
    print(f"\nè¯·é€‰æ‹©å®žéªŒæ¨¡å¼:")
    print("1. å•ä¸€FLQæ¨¡å¼æµ‹è¯•")
    print("2. å¤šç§FLQæ¨¡å¼å¯¹æ¯”")
    
    # choice = input("è¯·è¾“å…¥é€‰æ‹© (1 æˆ– 2): ").strip()
    choice = "2"
    
    if choice == "1":
        # å•ä¸€æ¨¡å¼æµ‹è¯•
        print(f"\nå½“å‰FLQæ¨¡å¼: {FLQ_MODE}")
        logger = run_federated_flq_experiment(FLQ_MODE)
    
    elif choice == "2":
        # å¤šæ¨¡å¼å¯¹æ¯”æµ‹è¯•
        results = compare_flq_modes()
    
    else:
        print("è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤æ¨¡å¼è¿è¡Œ...")
        logger = run_federated_flq_experiment(FLQ_MODE)
    
    print(f"\nðŸŽ‰ æ‰€æœ‰å®žéªŒå®Œæˆï¼ç»“æžœå·²ä¿å­˜å’Œå¯è§†åŒ–ã€‚")
