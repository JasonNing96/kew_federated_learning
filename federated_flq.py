#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆFLQè”é‚¦å­¦ä¹ æµ‹è¯•è„šæœ¬
åŸºäºmaster.pyå’Œworker.pyï¼Œæ•´åˆä¸ºå•æ–‡ä»¶è¿è¡Œ
ä½¿ç”¨çœŸå®MNISTæ•°æ®é›†æµ‹è¯•FLQé‡åŒ–ç®—æ³•æ•ˆæœ
"""

import numpy as np
import time
import random
import json
import os
from typing import List, Tuple, Dict, Any
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from torchvision import datasets, transforms

# ---------------------
# å…¨å±€é…ç½®å‚æ•°
# ---------------------
NUM_WORKERS = 15       # è”é‚¦å­¦ä¹ å®¢æˆ·ç«¯æ•°é‡
NUM_ROUNDS = 50         # è”é‚¦å­¦ä¹ è½®æ¬¡ï¼ˆå‡å°‘è¿‡åº¦è®­ç»ƒï¼‰
LOCAL_EPOCHS = 4      # æ¯ä¸ªå®¢æˆ·ç«¯çš„æœ¬åœ°è®­ç»ƒè½®æ•°ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
BATCH_SIZE = 64       # æœ¬åœ°è®­ç»ƒæ‰¹æ¬¡å¤§å°
LR = 0.001           # å­¦ä¹ ç‡ï¼ˆé™ä½å­¦ä¹ ç‡ï¼‰
SEED = 42            # éšæœºç§å­

# FLQé‡åŒ–é…ç½®
FLQ_MODE = "sign1"    # é‡åŒ–æ¨¡å¼: "off", "sign1", "int8", "4bit"

# FLQæ‡’æƒ°èšåˆå‚æ•°
FLQ_D = 10           # å†å²çª—å£å¤§å°
FLQ_C = 100          # å¼ºåˆ¶é€šä¿¡å‘¨æœŸ
FLQ_CK = 0.8         # æƒé‡è¡°å‡ç³»æ•°
FLQ_CL = 0.01        # L2æ­£åˆ™åŒ–ç³»æ•°

# MNISTæ•°æ®é›†é…ç½®
TRAIN_SIZE = 60000    # MNISTè®­ç»ƒé›†å¤§å°
TEST_SIZE = 10000     # MNISTæµ‹è¯•é›†å¤§å°

print(f"ğŸ“Š FLQè”é‚¦å­¦ä¹ æµ‹è¯•é…ç½®:")
print(f"  å®¢æˆ·ç«¯æ•°: {NUM_WORKERS}")
print(f"  è®­ç»ƒè½®æ¬¡: {NUM_ROUNDS}")
print(f"  é‡åŒ–æ¨¡å¼: {FLQ_MODE}")
print(f"  æœ¬åœ°è½®æ•°: {LOCAL_EPOCHS}")
print(f"  æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")

# è®¾ç½®éšæœºç§å­
np.random.seed(SEED)
random.seed(SEED)
torch.manual_seed(SEED)
rng = np.random.default_rng(SEED)

# è®¾å¤‡é…ç½®
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"  ä½¿ç”¨è®¾å¤‡: {device}")

# ---------------------
# PyTorch MNISTæ¨¡å‹å®šä¹‰
# ---------------------
class MNISTNet(nn.Module):
    """ç®€å•çš„MNISTåˆ†ç±»ç½‘ç»œï¼ˆå¢åŠ æ­£åˆ™åŒ–ï¼‰"""
    def __init__(self):
        super(MNISTNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, 3, 1)  # å‡å°‘å‚æ•°é‡
        self.conv2 = nn.Conv2d(16, 32, 3, 1)  # å‡å°‘å‚æ•°é‡
        self.dropout1 = nn.Dropout(0.2)   # é™ä½dropoutç‡
        self.dropout2 = nn.Dropout(0.3)   # é™ä½dropoutç‡
        self.fc1 = nn.Linear(4608, 64)     # å‡å°‘éšè—å±‚å¤§å°
        self.fc2 = nn.Linear(64, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)

def get_model_parameters_count(model):
    """è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡"""
    return sum(p.numel() for p in model.parameters())

def model_to_vector(model):
    """å°†æ¨¡å‹å‚æ•°è½¬æ¢ä¸ºä¸€ç»´å‘é‡"""
    vec = []
    for param in model.parameters():
        vec.append(param.data.cpu().numpy().flatten())
    return np.concatenate(vec)

def vector_to_model(vector, model):
    """å°†ä¸€ç»´å‘é‡è½¬æ¢å›æ¨¡å‹å‚æ•°"""
    start_idx = 0
    for param in model.parameters():
        param_length = param.numel()
        param_data = vector[start_idx:start_idx + param_length]
        param.data = torch.from_numpy(param_data.reshape(param.shape)).float().to(param.device)
        start_idx += param_length

# ---------------------
# MNISTæ•°æ®é›†åŠ è½½å’Œåˆ†å‰²
# ---------------------
def load_mnist_data():
    """åŠ è½½MNISTæ•°æ®é›†"""
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)
    
    return train_dataset, test_dataset

def split_dataset_for_workers_iid(dataset, num_workers):
    """
    ä¸ºå¤šä¸ªworkeråˆ†å‰²æ•°æ®é›†ï¼ˆå®Œå…¨IIDæ•°æ®åˆ†å¸ƒï¼‰
    æ¯ä¸ªworkeréƒ½åŒ…å«æ‰€æœ‰ç±»åˆ«ï¼Œä¸”ç±»åˆ«åˆ†å¸ƒç›¸åŒ
    """
    num_items = len(dataset)
    
    # è·å–æ ‡ç­¾
    if hasattr(dataset, 'targets'):
        labels = dataset.targets
    else:
        labels = [dataset[i][1] for i in range(len(dataset))]
    
    labels = np.array(labels)
    num_classes = len(np.unique(labels))
    
    # æŒ‰ç±»åˆ«ç»„ç»‡æ•°æ®ç´¢å¼•
    class_indices = {}
    for cls in range(num_classes):
        class_indices[cls] = np.where(labels == cls)[0]
    
    worker_datasets = []
    
    print("ğŸ“Š ä½¿ç”¨IIDï¼ˆç‹¬ç«‹åŒåˆ†å¸ƒï¼‰æ•°æ®åˆ†å‰²æ–¹å¼")
    
    for worker_id in range(num_workers):
        worker_indices = []
        
        # æ¯ä¸ªworkerä»æ¯ä¸ªç±»åˆ«ä¸­è·å¾—ç›¸åŒæ¯”ä¾‹çš„æ•°æ®
        for cls in range(num_classes):
            cls_data = class_indices[cls]
            # å°†è¯¥ç±»åˆ«çš„æ•°æ®å¹³å‡åˆ†é…ç»™æ‰€æœ‰workers
            start_idx = worker_id * len(cls_data) // num_workers
            end_idx = (worker_id + 1) * len(cls_data) // num_workers
            worker_indices.extend(cls_data[start_idx:end_idx])
        
        # éšæœºæ‰“ä¹±ç´¢å¼•ï¼Œä¿è¯æ•°æ®çš„éšæœºæ€§
        np.random.shuffle(worker_indices)
        worker_datasets.append(Subset(dataset, worker_indices))
        
        # ç»Ÿè®¡æ¯ä¸ªworkerçš„ç±»åˆ«åˆ†å¸ƒ
        worker_labels = [labels[i] for i in worker_indices]
        unique, counts = np.unique(worker_labels, return_counts=True)
        print(f"Worker {worker_id} IIDåˆ†å¸ƒ ({len(worker_indices)}æ ·æœ¬): {dict(zip(unique, counts))}")
    
    return worker_datasets

def split_dataset_for_workers(dataset, num_workers, alpha=0.5):
    """
    ä¸ºå¤šä¸ªworkeråˆ†å‰²æ•°æ®é›†ï¼ˆæ¨¡æ‹ŸNon-IIDæ•°æ®åˆ†å¸ƒï¼‰
    alpha: æ§åˆ¶æ•°æ®åˆ†å¸ƒçš„ä¸å‡åŒ€ç¨‹åº¦ï¼Œ0=å®Œå…¨Non-IIDï¼Œ1=å®Œå…¨IID
    """
    num_items = len(dataset)
    items_per_worker = num_items // num_workers
    
    # è·å–æ ‡ç­¾
    if hasattr(dataset, 'targets'):
        labels = dataset.targets
    else:
        labels = [dataset[i][1] for i in range(len(dataset))]
    
    labels = np.array(labels)
    num_classes = len(np.unique(labels))
    
    worker_datasets = []
    
    for worker_id in range(num_workers):
        if alpha == 1.0:
            # å®Œå…¨IIDï¼šéšæœºåˆ†é…
            indices = np.random.choice(num_items, items_per_worker, replace=False)
        else:
            # Non-IIDï¼šæ¯ä¸ªworkerä¸»è¦åŒ…å«ç‰¹å®šç±»åˆ«çš„æ•°æ®
            primary_classes = np.random.choice(num_classes, max(1, int(alpha * num_classes)), replace=False)
            
            # è·å–ä¸»è¦ç±»åˆ«çš„ç´¢å¼•
            primary_indices = []
            for cls in primary_classes:
                cls_indices = np.where(labels == cls)[0]
                primary_indices.extend(cls_indices[:items_per_worker // len(primary_classes)])
            
            # éšæœºæ·»åŠ å…¶ä»–ç±»åˆ«çš„å°‘é‡æ•°æ®
            remaining_count = items_per_worker - len(primary_indices)
            if remaining_count > 0:
                other_indices = np.setdiff1d(range(num_items), primary_indices)
                additional_indices = np.random.choice(other_indices, remaining_count, replace=False)
                primary_indices.extend(additional_indices)
            
            indices = primary_indices[:items_per_worker]
        
        worker_dataset = Subset(dataset, indices)
        worker_datasets.append(worker_dataset)
        
        # ç»Ÿè®¡æ¯ä¸ªworkerçš„ç±»åˆ«åˆ†å¸ƒ
        worker_labels = [labels[i] for i in indices]
        unique, counts = np.unique(worker_labels, return_counts=True)
        print(f"Worker {worker_id} æ•°æ®åˆ†å¸ƒ: {dict(zip(unique, counts))}")
    
    return worker_datasets

# ---------------------
# FLQé‡åŒ–ç®—æ³•ï¼ˆä»flq_quantization.pyç®€åŒ–ï¼‰
# ---------------------
def flq_relative_quantization(gradients, reference_gradients, bits):
    """
    FLQç›¸å¯¹é‡åŒ– - åŸºäºåŸå§‹ç®—æ³•çš„quantdå‡½æ•°
    Args:
        gradients: å½“å‰æ¢¯åº¦å‘é‡
        reference_gradients: å‚è€ƒæ¢¯åº¦å‘é‡(mgr[m, :])  
        bits: é‡åŒ–ä½æ•°
    """
    if reference_gradients is None:
        reference_gradients = np.zeros_like(gradients)
    
    # è®¡ç®—ç›¸å¯¹å·®å€¼
    diff = gradients - reference_gradients
    r = np.max(np.abs(diff))
    
    if r == 0:
        return gradients, 0.0, bits * len(gradients)
    
    # é‡åŒ–æ­¥é•¿
    delta = r / (np.floor(2 ** bits) - 1)
    
    # ç›¸å¯¹é‡åŒ–
    quantized_diff = reference_gradients - r + 2 * delta * np.floor((diff + r + delta) / (2 * delta))
    quantized = reference_gradients + quantized_diff
    
    # è®¡ç®—é‡åŒ–è¯¯å·®
    quantization_error = np.sum((quantized - gradients) ** 2)
    
    return quantized, quantization_error, bits * len(gradients)

def flq_sign_quantization(gradients):
    """FLQç¬¦å·é‡åŒ–ï¼ˆ1ä½ï¼‰"""
    signs = np.sign(gradients)
    scale_factor = np.mean(np.abs(gradients))
    quantized = signs * scale_factor
    logical_bits = len(gradients)  # æ¯å‚æ•°1ä½
    quantization_error = np.sum((quantized - gradients) ** 2)
    return quantized, quantization_error, logical_bits

def flq_8bit_quantization(gradients):
    """FLQ 8ä½é‡åŒ–"""
    max_abs = np.max(np.abs(gradients))
    if max_abs == 0:
        return gradients, 0.0, 8 * len(gradients)
    
    scale_factor = max_abs / 127.0
    quantized_int8 = np.clip(np.round(gradients / scale_factor), -128, 127)
    quantized = quantized_int8 * scale_factor
    logical_bits = 8 * len(gradients)
    quantization_error = np.sum((quantized - gradients) ** 2)
    return quantized, quantization_error, logical_bits

def flq_4bit_quantization(gradients):
    """FLQ 4ä½é‡åŒ–"""
    max_abs = np.max(np.abs(gradients))
    if max_abs == 0:
        return gradients, 0.0, 4 * len(gradients)
    
    scale_factor = max_abs / 7.0
    quantized_int4 = np.clip(np.round(gradients / scale_factor), -8, 7)
    quantized = quantized_int4 * scale_factor
    logical_bits = 4 * len(gradients)
    quantization_error = np.sum((quantized - gradients) ** 2)
    return quantized, quantization_error, logical_bits

def apply_flq_quantization(gradients, mode, reference_gradients=None):
    """åº”ç”¨FLQé‡åŒ–"""
    if mode == "off":
        return gradients, 0.0, 32 * len(gradients)
    elif mode == "sign1":
        return flq_sign_quantization(gradients)
    elif mode == "int8":
        if reference_gradients is not None:
            return flq_relative_quantization(gradients, reference_gradients, 8)
        else:
            return flq_8bit_quantization(gradients)
    elif mode == "4bit":
        if reference_gradients is not None:
            return flq_relative_quantization(gradients, reference_gradients, 4)
        else:
            return flq_4bit_quantization(gradients)
    else:
        raise ValueError(f"Unknown FLQ mode: {mode}")

# ---------------------
# å‚æ•°æœåŠ¡å™¨ç±»ï¼ˆåŸºäºmaster.pyï¼‰
# ---------------------
class FederatedMaster:
    """è”é‚¦å­¦ä¹ å‚æ•°æœåŠ¡å™¨ - å®ç°FLQæ‡’æƒ°èšåˆæœºåˆ¶"""
    
    def __init__(self, min_clients: int = 10, max_clients: int = 20):
        # åˆ›å»ºå…¨å±€æ¨¡å‹æ¥è·å–å‚æ•°ç»´åº¦
        global_model = MNISTNet()
        self.model_dim = get_model_parameters_count(global_model)
        self.global_weights = model_to_vector(global_model)
        
        self.min_clients = min_clients
        self.max_clients = max_clients
        self.global_round = 0
        
        # FLQæ‡’æƒ°èšåˆç›¸å…³çŠ¶æ€
        self.num_workers = max_clients
        self.aggregated_gradient = np.zeros(self.model_dim)  # dsa in original algorithm
        self.worker_communication_indicators = {}  # Ind[m, k] 
        self.worker_clocks = {}  # clock[m]
        self.worker_last_gradients = {}  # mgr[m, :]
        self.worker_last_errors = {}  # ehat[m]
        
        # å†å²å‚æ•°è·Ÿè¸ª (ç”¨äºè®¡ç®—me[m])
        self.parameter_history = np.zeros((self.model_dim, FLQ_D + 1))  # dtheta[:, k]
        self.ksi_weights = self._initialize_ksi_weights()
        
        self.workers_status = {}
        
        print(f"ğŸ›ï¸ FLQå‚æ•°æœåŠ¡å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"  æ¨¡å‹å‚æ•°é‡: {self.model_dim}")
        print(f"  æœ€å°å®¢æˆ·ç«¯æ•°: {min_clients}")
        print(f"  æ‡’æƒ°èšåˆçª—å£: {FLQ_D}")
        print(f"  å¼ºåˆ¶é€šä¿¡å‘¨æœŸ: {FLQ_C}")
    
    def _initialize_ksi_weights(self):
        """åˆå§‹åŒ–ksiæƒé‡çŸ©é˜µ"""
        ksi = np.ones((FLQ_D, FLQ_D + 1))
        for i in range(FLQ_D + 1):
            if i == 0:
                ksi[:, i] = np.ones(FLQ_D)
            elif i <= FLQ_D and i > 0:
                ksi[:, i] = (1.0 / i) * np.ones(FLQ_D)
        return FLQ_CK * ksi
    
    def get_global_model(self) -> Tuple[np.ndarray, int]:
        """è·å–å…¨å±€æ¨¡å‹"""
        return self.global_weights.copy(), self.global_round
    
    def receive_update(self, worker_id: str, round_id: int, gradient: np.ndarray, 
                      quantized_gradient: np.ndarray, quantization_error: float,
                      num_samples: int, loss: float) -> Dict[str, Any]:
        """
        æ¥æ”¶å®¢æˆ·ç«¯æ›´æ–° - å®ç°FLQæ‡’æƒ°èšåˆæœºåˆ¶
        
        Args:
            worker_id: å®¢æˆ·ç«¯ID
            round_id: è½®æ¬¡ID
            gradient: åŸå§‹æ¢¯åº¦å‘é‡
            quantized_gradient: é‡åŒ–åçš„æ¢¯åº¦å‘é‡
            quantization_error: é‡åŒ–è¯¯å·® e[m]
            num_samples: æ ·æœ¬æ•°é‡
            loss: è®­ç»ƒæŸå¤±
        """
        # éªŒè¯ç»´åº¦
        if gradient.shape[0] != self.model_dim:
            return {"status": "error", "msg": f"Dimension mismatch: {gradient.shape[0]} vs {self.model_dim}"}
        
        # åˆå§‹åŒ–workerçŠ¶æ€(å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡)
        if worker_id not in self.worker_clocks:
            self.worker_clocks[worker_id] = 0
            self.worker_last_gradients[worker_id] = np.zeros(self.model_dim)
            self.worker_last_errors[worker_id] = 0.0
            self.worker_communication_indicators[worker_id] = []
        
        # è®¡ç®—æ˜¯å¦éœ€è¦é€šä¿¡ (FLQæ‡’æƒ°èšåˆæ ¸å¿ƒé€»è¾‘)
        should_communicate = self._check_communication_condition(
            worker_id, round_id, quantized_gradient, quantization_error
        )
        
        # è®°å½•é€šä¿¡æŒ‡ç¤ºå™¨
        self.worker_communication_indicators[worker_id].append(should_communicate)
        
        # è®°å½•workerçŠ¶æ€
        self.workers_status[worker_id] = {
            "round": round_id,
            "num_samples": num_samples,
            "loss": loss,
            "should_communicate": should_communicate,
            "quantization_error": quantization_error,
            "timestamp": time.time()
        }
        
        if should_communicate:
            # æ›´æ–°workerçš„ä¸Šæ¬¡é€šä¿¡çŠ¶æ€
            self.worker_last_gradients[worker_id] = quantized_gradient.copy()
            self.worker_last_errors[worker_id] = quantization_error
            self.worker_clocks[worker_id] = 0
            
            # ç´¯ç§¯åˆ°å…¨å±€èšåˆæ¢¯åº¦ (dsa)
            gradient_diff = quantized_gradient - self.worker_last_gradients[worker_id]
            self.aggregated_gradient += gradient_diff
            
            print(f"ğŸ“¡ {worker_id} å‚ä¸é€šä¿¡ - Round {round_id}")
        else:
            # ä¸é€šä¿¡ï¼Œå¢åŠ æ—¶é’Ÿ
            self.worker_clocks[worker_id] += 1
            print(f"â¸ï¸ {worker_id} è·³è¿‡é€šä¿¡ - Round {round_id} (Clock: {self.worker_clocks[worker_id]})")
        
        return {
            "status": "ok",
            "should_communicate": should_communicate,
            "global_round": self.global_round
        }
    
    def _check_communication_condition(self, worker_id: str, round_id: int, 
                                     quantized_gradient: np.ndarray, quantization_error: float) -> bool:
        """
        æ£€æŸ¥FLQæ‡’æƒ°èšåˆé€šä¿¡æ¡ä»¶
        
        åŸå§‹æ¡ä»¶: ||dL[m]||Â² >= (1/(Î±Â²MÂ²)) * me[m] + 3 * (e[m] + ehat[m]) or clock[m] == C
        """
        # å¼ºåˆ¶é€šä¿¡æ¡ä»¶
        if self.worker_clocks[worker_id] >= FLQ_C:
            return True
        
        # è®¡ç®—æ¢¯åº¦å·®å€¼ dL[m] = gr[m] - mgr[m]
        last_gradient = self.worker_last_gradients[worker_id]
        gradient_diff = quantized_gradient - last_gradient
        gradient_diff_norm_sq = np.sum(gradient_diff ** 2)
        
        # è®¡ç®—å†å²å‚æ•°å˜åŒ–çš„åŠ¨æ€é˜ˆå€¼ me[m]
        me_threshold = self._calculate_dynamic_threshold(worker_id, round_id)
        
        # è®¡ç®—é€šä¿¡é˜ˆå€¼
        alpha = LR
        M = self.num_workers
        last_error = self.worker_last_errors[worker_id]
        
        communication_threshold = (1.0 / (alpha ** 2 * M ** 2)) * me_threshold + 3 * (quantization_error + last_error)
        
        # æ‡’æƒ°èšåˆæ¡ä»¶
        should_communicate = gradient_diff_norm_sq >= communication_threshold
        
        return should_communicate
    
    def _calculate_dynamic_threshold(self, worker_id: str, round_id: int) -> float:
        """
        è®¡ç®—åŠ¨æ€é˜ˆå€¼ me[m] - åŸºäºå†å²å‚æ•°å˜åŒ–
        
        åŸå§‹é€»è¾‘:
        for d in range(0, D):
            if (k - d >= 0):
                if (k <= D):
                    me[m] = me[m] + ksi[d, k] * dtheta[:, k - d].dot(dtheta[:, k - d])
                if (k > D):
                    me[m] = me[m] + ksi[d, D] * dtheta[:, k - d].dot(dtheta[:, k - d])
        """
        me_value = 0.0
        
        for d in range(FLQ_D):
            history_idx = round_id - d
            if history_idx >= 0:
                # è·å–å†å²å‚æ•°å˜åŒ–
                if history_idx < self.parameter_history.shape[1]:
                    parameter_change = self.parameter_history[:, history_idx]
                    parameter_change_norm_sq = np.sum(parameter_change ** 2)
                    
                    # é€‰æ‹©æƒé‡
                    if round_id <= FLQ_D:
                        weight = self.ksi_weights[d, round_id] if round_id < self.ksi_weights.shape[1] else 0.0
                    else:
                        weight = self.ksi_weights[d, FLQ_D] if FLQ_D < self.ksi_weights.shape[1] else 0.0
                    
                    me_value += weight * parameter_change_norm_sq
        
        return me_value
    
    def apply_aggregated_update(self):
        """åº”ç”¨èšåˆçš„æ¢¯åº¦æ›´æ–°åˆ°å…¨å±€æ¨¡å‹"""
        # æ›´æ–°å…¨å±€æƒé‡
        previous_weights = self.global_weights.copy()
        self.global_weights = self.global_weights + self.aggregated_gradient
        
        # æ›´æ–°å‚æ•°å†å² (dtheta[:, k] = current_weights - previous_weights)
        parameter_change = self.global_weights - previous_weights
        
        # æ»‘åŠ¨çª—å£æ›´æ–°å†å²
        if self.global_round < FLQ_D:
            self.parameter_history[:, self.global_round] = parameter_change
        else:
            # å·¦ç§»å†å²å¹¶æ·»åŠ æ–°çš„å˜åŒ–
            self.parameter_history[:, :-1] = self.parameter_history[:, 1:]
            self.parameter_history[:, -1] = parameter_change
        
        # é‡ç½®èšåˆæ¢¯åº¦
        self.aggregated_gradient = np.zeros(self.model_dim)
        
        # æ›´æ–°è½®æ¬¡
        self.global_round += 1
        
        # ç»Ÿè®¡é€šä¿¡å®¢æˆ·ç«¯æ•°é‡
        communicating_workers = sum(1 for status in self.workers_status.values() 
                                  if status.get("should_communicate", False))
        
        print(f"ğŸ”„ ç¬¬{self.global_round-1}è½®èšåˆå®Œæˆï¼Œé€šä¿¡å®¢æˆ·ç«¯: {communicating_workers}/{len(self.workers_status)}")
        
        return True

# ---------------------
# è”é‚¦å­¦ä¹ å®¢æˆ·ç«¯ç±»ï¼ˆåŸºäºworker.pyï¼‰
# ---------------------
class FederatedWorker:
    """è”é‚¦å­¦ä¹ å®¢æˆ·ç«¯"""
    
    def __init__(self, worker_id: str, dataset, test_dataset):
        self.worker_id = worker_id
        self.dataset = dataset
        self.test_dataset = test_dataset
        self.dataset_size = len(dataset)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.train_loader = DataLoader(
            dataset, 
            batch_size=BATCH_SIZE, 
            shuffle=True,
            drop_last=True
        )
        
        # åˆ›å»ºæœ¬åœ°æ¨¡å‹
        self.model = MNISTNet().to(device)
        self.criterion = nn.NLLLoss()
        
        print(f"ğŸ‘¤ å®¢æˆ·ç«¯ {worker_id} åˆå§‹åŒ–å®Œæˆ")
        print(f"  æœ¬åœ°æ•°æ®é‡: {self.dataset_size}")
        print(f"  æ¨¡å‹å‚æ•°é‡: {get_model_parameters_count(self.model)}")
    
    def local_training(self, global_weights: np.ndarray, epochs: int, lr: float) -> Tuple[np.ndarray, float, float]:
        """çœŸå®çš„æœ¬åœ°MNISTè®­ç»ƒ"""
        # 1. è®¾ç½®å…¨å±€æƒé‡åˆ°æœ¬åœ°æ¨¡å‹
        vector_to_model(global_weights, self.model)
        initial_weights = global_weights.copy()
        
        # 2. åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆé™ä½momentumé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
        optimizer = optim.SGD(self.model.parameters(), lr=lr, momentum=0.5)
        
        # 3. æœ¬åœ°è®­ç»ƒï¼ˆæ·»åŠ è®­ç»ƒå™ªå£°æ¨¡æ‹ŸçœŸå®ç¯å¢ƒï¼‰
        self.model.train()
        total_loss = 0.0
        total_samples = 0
        correct = 0
        
        # æ·»åŠ å°‘é‡å‚æ•°å™ªå£°æ¨¡æ‹Ÿç³»ç»Ÿå¼‚è´¨æ€§
        with torch.no_grad():
            for param in self.model.parameters():
                noise = torch.randn_like(param) * 0.001  # å°å™ªå£°
                param.add_(noise)
        
        for epoch in range(epochs):
            epoch_loss = 0.0
            epoch_samples = 0
            
            for batch_idx, (data, target) in enumerate(self.train_loader):
                data, target = data.to(device), target.to(device)
                
                optimizer.zero_grad()
                output = self.model(data)
                loss = self.criterion(output, target)
                loss.backward()
                optimizer.step()
                
                # ç»Ÿè®¡
                epoch_loss += loss.item()
                epoch_samples += len(data)
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
            
            total_loss += epoch_loss
            total_samples += epoch_samples
            
            if epoch == 0:  # åªåœ¨ç¬¬ä¸€ä¸ªepochæ‰“å°è¯¦ç»†ä¿¡æ¯
                print(f"  [{self.worker_id}] Epoch {epoch+1}/{epochs}: "
                      f"Loss={epoch_loss/len(self.train_loader):.4f}")
        
        # 4. è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        avg_loss = total_loss / (epochs * len(self.train_loader))
        accuracy = correct / total_samples if total_samples > 0 else 0.0
        
        # 5. è®¡ç®—æƒé‡æ›´æ–°
        final_weights = model_to_vector(self.model)
        delta = final_weights - initial_weights
        
        return delta, avg_loss, accuracy
    
    def evaluate_model(self, global_weights: np.ndarray = None) -> Tuple[float, float]:
        """è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½"""
        if global_weights is not None:
            vector_to_model(global_weights, self.model)
        
        self.model.eval()
        test_loss = 0
        correct = 0
        total = 0
        
        # ä½¿ç”¨å®Œæ•´æµ‹è¯•é›†ï¼Œä½†éšæœºé‡‡æ ·2000ä¸ªæ ·æœ¬é¿å…æ•°æ®æ³„éœ²
        test_indices = np.random.choice(len(self.test_dataset), min(2000, len(self.test_dataset)), replace=False)
        test_subset = Subset(self.test_dataset, test_indices)
        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False)
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                output = self.model(data)
                test_loss += self.criterion(output, target).item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += len(data)
        
        avg_loss = test_loss / len(test_loader)
        accuracy = correct / total if total > 0 else 0.0
        
        return avg_loss, accuracy
    
    def participate_round(self, master: FederatedMaster, flq_mode: str) -> Dict[str, Any]:
        """å‚ä¸ä¸€è½®è”é‚¦å­¦ä¹  - æ”¯æŒFLQæ‡’æƒ°èšåˆ"""
        # 1. ä»æœåŠ¡å™¨è·å–å…¨å±€æ¨¡å‹
        global_weights, current_round = master.get_global_model()
        
        # 2. æœ¬åœ°è®­ç»ƒ
        delta, train_loss, train_accuracy = self.local_training(global_weights, LOCAL_EPOCHS, LR)
        
        # 3. è·å–å‚è€ƒæ¢¯åº¦ç”¨äºç›¸å¯¹é‡åŒ–
        reference_gradient = None
        if self.worker_id in master.worker_last_gradients:
            reference_gradient = master.worker_last_gradients[self.worker_id]
        
        # 4. FLQé‡åŒ–
        quantized_delta, quantization_error, logical_bits = apply_flq_quantization(
            delta, flq_mode, reference_gradient
        )
        
        # 5. è®¡ç®—é€šä¿¡å¼€é”€
        original_bits = 32 * len(delta)  # åŸå§‹32ä½æµ®ç‚¹æ•°
        compression_ratio = original_bits / logical_bits if logical_bits > 0 else 1.0
        
        # 6. ä½¿ç”¨å®é™…å‚ä¸çš„æ ·æœ¬æ•°é‡
        num_samples = self.dataset_size
        
        # 7. ä¸ŠæŠ¥ç»™æœåŠ¡å™¨ (åŒ…å«åŸå§‹æ¢¯åº¦ã€é‡åŒ–æ¢¯åº¦å’Œé‡åŒ–è¯¯å·®)
        resp = master.receive_update(
            worker_id=self.worker_id,
            round_id=current_round,
            gradient=delta,
            quantized_gradient=quantized_delta,
            quantization_error=quantization_error,
            num_samples=num_samples,
            loss=train_loss
        )
        
        # 8. æå–é€šä¿¡å†³ç­–
        should_communicate = resp.get("should_communicate", True)
        actual_bits = logical_bits if should_communicate else 0  # ä¸é€šä¿¡åˆ™æ²¡æœ‰å®é™…ä¼ è¾“
        
        return {
            "round": current_round,
            "train_loss": train_loss,
            "train_accuracy": train_accuracy,
            "num_samples": num_samples,
            "logical_bits": actual_bits,  # å®é™…ä¼ è¾“çš„bits
            "compression_ratio": compression_ratio,
            "should_communicate": should_communicate,
            "quantization_error": quantization_error,
            "response": resp
        }

# ---------------------
# å®éªŒç»“æœè®°å½•å’Œåˆ†æ
# ---------------------
class ExperimentLogger:
    """å®éªŒæ—¥å¿—è®°å½•å™¨ - ç®€å•æ–‡æœ¬æ ¼å¼å¿«é€ŸéªŒè¯"""
    
    def __init__(self, experiment_name: str = "flq_experiment"):
        self.experiment_name = experiment_name
        self.results = []
        self.communication_costs = []
        self.accuracies = []
        
        # åˆ›å»ºExcelæ—¥å¿—æ–‡ä»¶
        import time
        import pandas as pd
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        self.log_file = f"logs/flq_{experiment_name}_{timestamp}.xlsx"
        os.makedirs("logs", exist_ok=True)
        
        # åˆ›å»ºExcelå·¥ä½œç°¿ï¼Œå…ˆä¿å­˜é…ç½®ä¿¡æ¯
        self.excel_data = []
        
        # åˆ›å»ºé…ç½®ä¿¡æ¯
        config_info = {
            'round': 'CONFIG',
            'global_acc': f'workers={NUM_WORKERS}',
            'global_loss': f'rounds={NUM_ROUNDS}',
            'avg_train_acc': f'epochs={LOCAL_EPOCHS}',
            'avg_train_loss': f'lr={LR}',
            'comm_bits': f'time={time.strftime("%Y-%m-%d %H:%M:%S")}',
            'comm_workers': '',
            'comm_rate': ''
        }
        self.excel_data.append(config_info)
        
        # æ·»åŠ è¡¨å¤´
        header = {
            'round': 'round',
            'global_acc': 'global_acc',
            'global_loss': 'global_loss',
            'avg_train_acc': 'avg_train_acc',
            'avg_train_loss': 'avg_train_loss',
            'comm_bits': 'comm_bits',
            'comm_workers': 'comm_workers',
            'comm_rate': 'comm_rate'
        }
        self.excel_data.append(header)
        
        print(f"ğŸ“Š Excelæ—¥å¿—æ–‡ä»¶: {self.log_file}")
        
        # æ ¹æ®è®ºæ–‡å›¾ç‰‡è®¾è®¡çš„æ•°æ®ç»“æ„
        self.experiment_data = {
            "experiment_info": {
                "name": experiment_name,
                "timestamp": time.strftime("%Y-%m-%d_%H-%M-%S"),
                "config": {
                    "num_workers": NUM_WORKERS,
                    "num_rounds": NUM_ROUNDS,
                    "local_epochs": LOCAL_EPOCHS,
                    "batch_size": BATCH_SIZE,
                    "learning_rate": LR,
                    "flq_mode": FLQ_MODE
                }
            },
            "convergence_data": {
                "rounds": [],
                "global_test_loss": [],
                "global_test_accuracy": [],
                "average_train_loss": [],
                "average_train_accuracy": [],
                "communication_bits": [],
                "compression_ratios": []
            },
            "communication_data": {
                "total_upload_bits": 0,
                "total_broadcast_bits": 0,
                "per_round_bits": [],
                "quantization_levels": []
            },
            "quantization_analysis": {
                "mode": FLQ_MODE,
                "rmse_values": [],
                "approximation_quality": [],
                "binary_values": []
            },
            "resource_optimization": {
                "final_accuracy": 0.0,
                "total_iterations": 0,
                "total_communication_bits": 0,
                "convergence_round": -1
            }
        }
    
    def log_round(self, round_id: int, worker_results: List[Dict], global_loss: float, global_accuracy: float = 0.0):
        """è®°å½•ä¸€è½®å®éªŒç»“æœ - ç®€å•æ–‡æœ¬æ ¼å¼"""
        # èšåˆæœ¬è½®ç»Ÿè®¡
        total_bits = sum(r["logical_bits"] for r in worker_results)  # å®é™…ä¼ è¾“çš„bits
        avg_compression = np.mean([r["compression_ratio"] for r in worker_results])
        avg_train_loss = np.mean([r["train_loss"] for r in worker_results])
        avg_train_accuracy = np.mean([r["train_accuracy"] for r in worker_results])
        
        # FLQç‰¹æœ‰ç»Ÿè®¡
        communicating_workers = sum(1 for r in worker_results if r.get("should_communicate", True))
        communication_rate = communicating_workers / len(worker_results) if worker_results else 0.0
        
        round_result = {
            "round": round_id,
            "global_loss": global_loss,
            "global_accuracy": global_accuracy,
            "avg_train_loss": avg_train_loss,
            "avg_train_accuracy": avg_train_accuracy,
            "total_communication_bits": total_bits,
            "avg_compression_ratio": avg_compression,
            "num_workers": len(worker_results),
            "communicating_workers": communicating_workers,
            "communication_rate": communication_rate
        }
        
        self.results.append(round_result)
        self.communication_costs.append(total_bits)
        self.accuracies.append(global_accuracy)
        
        # æ·»åŠ æ•°æ®åˆ°Excelè®°å½•
        excel_row = {
            'round': round_id,
            'global_acc': global_accuracy,
            'global_loss': global_loss,
            'avg_train_acc': avg_train_accuracy,
            'avg_train_loss': avg_train_loss,
            'comm_bits': total_bits,
            'comm_workers': communicating_workers,
            'comm_rate': communication_rate
        }
        self.excel_data.append(excel_row)
        
        # ä¿å­˜åˆ°Excelæ–‡ä»¶
        self._save_excel()
        
        # ç®€åŒ–æ§åˆ¶å°è¾“å‡º
        print(f"R{round_id:2d}: acc={global_accuracy:.3f}, loss={global_loss:.4f}, comm={communicating_workers}/{len(worker_results)}")
        
        # æ¯10è½®è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        if round_id % 10 == 0 or round_id < 5:
            print(f"ğŸ“ˆ ç¬¬{round_id}è½®è¯¦ç»†ç»“æœ:")
            print(f"  å…¨å±€æµ‹è¯•æŸå¤±: {global_loss:.4f}")
            print(f"  å…¨å±€æµ‹è¯•ç²¾åº¦: {global_accuracy:.3f}")
            print(f"  å¹³å‡è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")
            print(f"  å¹³å‡è®­ç»ƒç²¾åº¦: {avg_train_accuracy:.3f}")
            print(f"  é€šä¿¡å¼€é”€: {total_bits:,} bits")
            print(f"  é€šä¿¡å®¢æˆ·ç«¯: {communicating_workers}/{len(worker_results)} ({communication_rate:.1%})")
            print(f"  å¹³å‡å‹ç¼©æ¯”: {avg_compression:.1f}:1")
    
    def _save_excel(self):
        """ä¿å­˜æ•°æ®åˆ°Excelæ–‡ä»¶"""
        try:
            import pandas as pd
            df = pd.DataFrame(self.excel_data)
            df.to_excel(self.log_file, index=False, sheet_name='FLQ_Results')
        except ImportError:
            # å¦‚æœæ²¡æœ‰pandasï¼Œå›é€€åˆ°CSVæ ¼å¼
            import csv
            csv_file = self.log_file.replace('.xlsx', '.csv')
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                if self.excel_data:
                    writer = csv.DictWriter(f, fieldnames=self.excel_data[0].keys())
                    writer.writeheader()
                    for row in self.excel_data[2:]:  # è·³è¿‡é…ç½®è¡Œå’Œè¡¨å¤´è¡Œ
                        writer.writerow(row)
            print(f"âš ï¸ pandasæœªå®‰è£…ï¼Œå·²ä¿å­˜ä¸ºCSVæ ¼å¼: {csv_file}")
        except Exception as e:
            print(f"âš ï¸ Excelä¿å­˜å¤±è´¥: {e}")
    
    def _update_json_data(self, round_id: int, worker_results: List[Dict], global_loss: float, 
                         global_accuracy: float, total_bits: int, avg_compression: float,
                         avg_train_loss: float, avg_train_accuracy: float):
        """æ›´æ–°JSONæ•°æ®ç»“æ„"""
        # æ”¶æ•›æ•°æ® (å¯¹åº”è®ºæ–‡å›¾2)
        conv_data = self.experiment_data["convergence_data"]
        conv_data["rounds"].append(round_id)
        conv_data["global_test_loss"].append(float(global_loss))
        conv_data["global_test_accuracy"].append(float(global_accuracy))
        conv_data["average_train_loss"].append(float(avg_train_loss))
        conv_data["average_train_accuracy"].append(float(avg_train_accuracy))
        conv_data["communication_bits"].append(int(total_bits))
        conv_data["compression_ratios"].append(float(avg_compression))
        
        # é€šä¿¡æ•°æ® (å¯¹åº”è®ºæ–‡å›¾3å’Œè¡¨1)
        comm_data = self.experiment_data["communication_data"]
        comm_data["per_round_bits"].append(int(total_bits))
        comm_data["total_upload_bits"] += total_bits  # å®¢æˆ·ç«¯ä¸Šä¼ 
        comm_data["total_broadcast_bits"] += total_bits // len(worker_results)  # æœåŠ¡å™¨å¹¿æ’­
        
        # é‡åŒ–åˆ†æ (å¯¹åº”è®ºæ–‡å›¾4)
        quant_data = self.experiment_data["quantization_analysis"]
        if FLQ_MODE == "sign1":
            # è®¡ç®—äºŒè¿›åˆ¶å€¼ (0æˆ–1)
            binary_vals = [1.0 if r["compression_ratio"] > 1.0 else 0.0 for r in worker_results]
            quant_data["binary_values"].extend(binary_vals)
        
        # è®¡ç®—RMSE (è¿‘ä¼¼è¯¯å·®)
        if len(worker_results) > 0:
            # ç®€åŒ–çš„RMSEè®¡ç®—ï¼šåŸºäºå‹ç¼©æ¯”çš„è¿‘ä¼¼è¯¯å·®
            rmse = 1.0 / avg_compression if avg_compression > 0 else 1.0
            quant_data["rmse_values"].append(float(rmse))
            quant_data["approximation_quality"].append(float(global_accuracy))
        
        # èµ„æºä¼˜åŒ–æ±‡æ€» (å¯¹åº”è®ºæ–‡è¡¨1)
        resource_data = self.experiment_data["resource_optimization"]
        resource_data["final_accuracy"] = float(global_accuracy)
        resource_data["total_iterations"] = round_id + 1
        resource_data["total_communication_bits"] = comm_data["total_upload_bits"]
        
        # æ£€æµ‹æ”¶æ•›ç‚¹ï¼ˆæŸå¤±å˜åŒ–å°äºé˜ˆå€¼ï¼‰
        if (len(conv_data["global_test_loss"]) > 5 and 
            resource_data["convergence_round"] == -1):
            recent_losses = conv_data["global_test_loss"][-5:]
            if max(recent_losses) - min(recent_losses) < 0.01:  # æ”¶æ•›é˜ˆå€¼
                resource_data["convergence_round"] = round_id
    
    def plot_results(self, flq_mode: str):
        """ç»˜åˆ¶å®éªŒç»“æœ"""
        if not self.results:
            print("âš ï¸ æ²¡æœ‰å®éªŒæ•°æ®å¯ç»˜åˆ¶")
            return
        
        rounds = [r["round"] for r in self.results]
        global_losses = [r["global_loss"] for r in self.results]
        global_accuracies = [r["global_accuracy"] for r in self.results]
        comm_costs = [r["total_communication_bits"] / 1e6 for r in self.results]  # è½¬æ¢ä¸ºMB
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # æŸå¤±æ›²çº¿
        ax1.plot(rounds, global_losses, 'b-o', linewidth=2, markersize=4)
        ax1.set_xlabel('è”é‚¦å­¦ä¹ è½®æ¬¡')
        ax1.set_ylabel('å…¨å±€æµ‹è¯•æŸå¤±')
        ax1.set_title(f'FLQ-{flq_mode} æŸå¤±æ”¶æ•›æ›²çº¿')
        ax1.grid(True, alpha=0.3)
        
        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(rounds, global_accuracies, 'g-o', linewidth=2, markersize=4)
        ax2.set_xlabel('è”é‚¦å­¦ä¹ è½®æ¬¡')
        ax2.set_ylabel('å…¨å±€æµ‹è¯•å‡†ç¡®ç‡')
        ax2.set_title(f'FLQ-{flq_mode} å‡†ç¡®ç‡æ›²çº¿')
        ax2.grid(True, alpha=0.3)
        ax2.set_ylim(0, 1)
        
        # é€šä¿¡å¼€é”€
        ax3.plot(rounds, comm_costs, 'r-s', linewidth=2, markersize=4)
        ax3.set_xlabel('è”é‚¦å­¦ä¹ è½®æ¬¡')
        ax3.set_ylabel('é€šä¿¡å¼€é”€ (Mbits)')
        ax3.set_title(f'FLQ-{flq_mode} é€šä¿¡å¼€é”€')
        ax3.grid(True, alpha=0.3)
        
        # å‡†ç¡®ç‡-é€šä¿¡å¼€é”€æ•ˆç‡å›¾
        ax4.plot(comm_costs, global_accuracies, 'm-d', linewidth=2, markersize=4)
        ax4.set_xlabel('ç´¯è®¡é€šä¿¡å¼€é”€ (Mbits)')
        ax4.set_ylabel('å…¨å±€æµ‹è¯•å‡†ç¡®ç‡')
        ax4.set_title(f'FLQ-{flq_mode} é€šä¿¡æ•ˆç‡')
        ax4.grid(True, alpha=0.3)
        ax4.set_ylim(0, 1)
        
        plt.tight_layout()
        plt.savefig(f'flq_{flq_mode}_results.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“Š å®éªŒç»“æœå›¾å·²ä¿å­˜ä¸º flq_{flq_mode}_results.png")
    
    def save_experiment_data(self, filename: str = None):
        """ä¿å­˜Excelæ ¼å¼çš„å®éªŒæ•°æ®å’Œæ€»ç»“"""
        if not self.results:
            print("âš ï¸ æ²¡æœ‰å®éªŒæ•°æ®å¯ä¿å­˜")
            return None
            
        # æœ€ç»ˆä¿å­˜Excelæ–‡ä»¶
        self._save_excel()
        
        # ç”Ÿæˆæ–‡æœ¬æ€»ç»“
        final_result = self.results[-1]
        total_rounds = len(self.results)
        
        summary_file = self.log_file.replace('.xlsx', '_summary.txt').replace('.csv', '_summary.txt')
        
        try:
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write(f"FLQå®éªŒæ€»ç»“ - {self.experiment_name}\n")
                f.write("=" * 50 + "\n")
                f.write(f"é…ç½®: workers={NUM_WORKERS}, rounds={NUM_ROUNDS}, epochs={LOCAL_EPOCHS}, lr={LR}\n")
                f.write(f"æ€»è½®æ¬¡: {total_rounds}\n")
                f.write(f"æœ€ç»ˆå…¨å±€ç²¾åº¦: {final_result['global_accuracy']:.4f}\n")
                f.write(f"æœ€ç»ˆå…¨å±€æŸå¤±: {final_result['global_loss']:.4f}\n")
                f.write(f"æœ€ç»ˆè®­ç»ƒç²¾åº¦: {final_result['avg_train_accuracy']:.4f}\n")
                f.write(f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {final_result['avg_train_loss']:.4f}\n")
                f.write(f"æ€»é€šä¿¡å¼€é”€: {sum(self.communication_costs):,} bits\n")
                f.write(f"å¹³å‡é€šä¿¡ç‡: {np.mean([r['communication_rate'] for r in self.results]):.3f}\n")
                f.write("\næ”¶æ•›è¶‹åŠ¿åˆ†æ:\n")
                
                # ç®€å•çš„æ”¶æ•›åˆ†æ
                if len(self.results) >= 10:
                    early_acc = np.mean([r['global_accuracy'] for r in self.results[:5]])
                    late_acc = np.mean([r['global_accuracy'] for r in self.results[-5:]])
                    f.write(f"åˆæœŸç²¾åº¦ (å‰5è½®): {early_acc:.4f}\n")
                    f.write(f"åæœŸç²¾åº¦ (å5è½®): {late_acc:.4f}\n")
                    f.write(f"ç²¾åº¦æå‡: {late_acc - early_acc:.4f}\n")
            
            print(f"ğŸ“Š Excelæ•°æ®å·²ä¿å­˜: {self.log_file}")
            print(f"ğŸ“„ å®éªŒæ€»ç»“å·²ä¿å­˜: {summary_file}")
            return self.log_file
        except Exception as e:
            print(f"âŒ ä¿å­˜å®éªŒæ€»ç»“å¤±è´¥: {e}")
            return self.log_file  # è‡³å°‘è¿”å›Excelæ–‡ä»¶è·¯å¾„
    
    def load_experiment_data(self, filepath: str):
        """ä»JSONæ–‡ä»¶åŠ è½½å®éªŒæ•°æ®"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                self.experiment_data = json.load(f)
            print(f"ğŸ“„ å®éªŒæ•°æ®å·²ä» {filepath} åŠ è½½")
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½å®éªŒæ•°æ®å¤±è´¥: {e}")
            return False
    
    def get_summary_for_comparison(self):
        """è·å–ç”¨äºå¯¹æ¯”çš„å®éªŒæ‘˜è¦æ•°æ®"""
        resource_data = self.experiment_data["resource_optimization"]
        conv_data = self.experiment_data["convergence_data"]
        comm_data = self.experiment_data["communication_data"]
        
        return {
            "mode": self.experiment_data["quantization_analysis"]["mode"],
            "final_accuracy": resource_data["final_accuracy"],
            "final_loss": conv_data["global_test_loss"][-1] if conv_data["global_test_loss"] else 0.0,
            "total_communication_bits": resource_data["total_communication_bits"],
            "convergence_round": resource_data["convergence_round"],
            "total_iterations": resource_data["total_iterations"],
            "avg_compression_ratio": np.mean(conv_data["compression_ratios"]) if conv_data["compression_ratios"] else 1.0
        }

# ---------------------
# ä¸»å®éªŒå‡½æ•°
# ---------------------
def run_federated_flq_experiment(flq_mode: str = "sign1"):
    """è¿è¡ŒFLQè”é‚¦å­¦ä¹ å®éªŒ"""
    print(f"\nğŸš€ å¼€å§‹FLQ-{flq_mode}è”é‚¦å­¦ä¹ å®éªŒ")
    print("=" * 50)
    
    # 1. åŠ è½½å’Œåˆ†å‰²MNISTæ•°æ®é›†
    print("ğŸ“¥ åŠ è½½MNISTæ•°æ®é›†...")
    train_dataset, test_dataset = load_mnist_data()
    # ä½¿ç”¨æ›´å¼ºçš„Non-IIDåˆ†å¸ƒï¼Œalpha=0.1è¡¨ç¤ºæ›´ä¸å‡åŒ€çš„æ•°æ®åˆ†å¸ƒ
    worker_datasets = split_dataset_for_workers(train_dataset, NUM_WORKERS, alpha=0.1)
    
    # 2. åˆå§‹åŒ–å‚æ•°æœåŠ¡å™¨
    master = FederatedMaster(min_clients=5, max_clients=NUM_WORKERS)
    
    # 3. åˆå§‹åŒ–å®¢æˆ·ç«¯
    workers = []
    for i in range(NUM_WORKERS):
        worker = FederatedWorker(f"worker_{i}", worker_datasets[i], test_dataset)
        workers.append(worker)
    
    # 4. åˆå§‹åŒ–å®éªŒè®°å½•å™¨
    experiment_name = f"flq_{flq_mode}_federated_learning"
    logger = ExperimentLogger(experiment_name)
    
    # 5. å¼€å§‹è”é‚¦å­¦ä¹ è®­ç»ƒ
    for round_id in range(NUM_ROUNDS):
        print(f"\nğŸ”„ ç¬¬{round_id}è½®è”é‚¦å­¦ä¹ å¼€å§‹...")
        
        # æ‰€æœ‰å®¢æˆ·ç«¯å‚ä¸è®­ç»ƒ
        worker_results = []
        for worker in workers:
            result = worker.participate_round(master, flq_mode)
            worker_results.append(result)
            
            comm_status = "ğŸ”—" if result['should_communicate'] else "â¸ï¸"
            print(f"  {comm_status} {worker.worker_id}: train_loss={result['train_loss']:.4f}, "
                  f"train_acc={result['train_accuracy']:.3f}, "
                  f"compression={result['compression_ratio']:.1f}:1")
        
        # åº”ç”¨FLQèšåˆæ›´æ–°
        master.apply_aggregated_update()
        
        # è¯„ä¼°å…¨å±€æ¨¡å‹æ€§èƒ½
        global_weights, _ = master.get_global_model()
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªworkeræ¥è¯„ä¼°å…¨å±€æ¨¡å‹ï¼ˆæ‰€æœ‰workerå…±äº«æµ‹è¯•é›†ï¼‰
        test_loss, test_accuracy = workers[0].evaluate_model(global_weights)
        
        print(f"ğŸ¯ å…¨å±€æ¨¡å‹è¯„ä¼°: test_loss={test_loss:.4f}, test_acc={test_accuracy:.3f}")
        
        # è®°å½•å®éªŒç»“æœ
        logger.log_round(round_id, worker_results, test_loss, test_accuracy)
        
        # çŸ­æš‚é—´éš”
        time.sleep(0.1)
    
    print(f"\nâœ… FLQ-{flq_mode}è”é‚¦å­¦ä¹ å®éªŒå®Œæˆï¼")
    
    # 6. ç”Ÿæˆå®éªŒæŠ¥å‘Š
    final_loss = logger.results[-1]["global_loss"]
    final_accuracy = logger.results[-1]["global_accuracy"]
    total_comm = sum(logger.communication_costs) / 1e6  # è½¬æ¢ä¸ºMbits
    avg_compression = np.mean([r["avg_compression_ratio"] for r in logger.results])
    
    print(f"\nğŸ“Š å®éªŒæ€»ç»“:")
    print(f"  æœ€ç»ˆå…¨å±€æŸå¤±: {final_loss:.4f}")
    print(f"  æœ€ç»ˆå…¨å±€å‡†ç¡®ç‡: {final_accuracy:.3f}")
    print(f"  æ€»é€šä¿¡å¼€é”€: {total_comm:.2f} Mbits")
    print(f"  å¹³å‡å‹ç¼©æ¯”: {avg_compression:.1f}:1")
    
    # 6. ä¿å­˜Excelæ ¼å¼æ—¥å¿—
    excel_filepath = logger.save_experiment_data()
    if excel_filepath:
        print(f"âœ… å®éªŒç»“æœå·²ä¿å­˜ä¸ºExcelæ ¼å¼")
        
    # 7. æ˜¾ç¤ºå¿«é€ŸéªŒè¯ä¿¡æ¯
    print(f"\nğŸ” å¿«é€ŸéªŒè¯ç»“æœ:")
    print(f"  æ˜¯å¦åˆç†? ç²¾åº¦ {final_accuracy:.3f} ({'âœ… åˆç†' if 0.7 <= final_accuracy <= 0.95 else 'âŒ å¼‚å¸¸'})")
    print(f"  æ˜¯å¦åˆç†? æŸå¤± {final_loss:.3f} ({'âœ… åˆç†' if 0.05 <= final_loss <= 0.5 else 'âŒ å¼‚å¸¸'})")
    
    if final_accuracy > 0.95:
        print("âš ï¸ è­¦å‘Š: ç²¾åº¦è¿‡é«˜ï¼Œå¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆæˆ–æ•°æ®æ³„éœ²")
    if final_loss < 0.05:
        print("âš ï¸ è­¦å‘Š: æŸå¤±è¿‡ä½ï¼Œå¯èƒ½å­˜åœ¨è®­ç»ƒé—®é¢˜")
        
    # 8. è¾“å‡ºæ–‡ä»¶ä½ç½®æ–¹ä¾¿æŸ¥çœ‹
    print(f"\nğŸ“ æŸ¥çœ‹è¯¦ç»†ç»“æœ:")
    print(f"  ğŸ“Š Excelæ•°æ®: {logger.log_file}")
    if excel_filepath:
        summary_file = excel_filepath.replace('.xlsx', '_summary.txt').replace('.csv', '_summary.txt')
        print(f"  ğŸ“„ å®éªŒæ€»ç»“: {summary_file}")
    print(f"  ğŸ’¡ å»ºè®®: åœ¨Excelä¸­æ‰“å¼€æ•°æ®æ–‡ä»¶è¿›è¡Œåˆ†æå’Œç»˜å›¾")
    
    return logger

# ---------------------
# å¯¹æ¯”å®éªŒå‡½æ•°
# ---------------------
def compare_flq_modes():
    """å¯¹æ¯”ä¸åŒFLQé‡åŒ–æ¨¡å¼"""
    modes = ["off", "sign1", "int8", "4bit"]
    results = {}
    
    print(f"\nğŸ”¬ å¼€å§‹FLQé‡åŒ–æ¨¡å¼å¯¹æ¯”å®éªŒ")
    print("=" * 60)
    
    for mode in modes:
        print(f"\n{'='*20} FLQ-{mode} {'='*20}")
        logger = run_federated_flq_experiment(mode)
        results[mode] = logger
    
    # æ±‡æ€»å¯¹æ¯”ç»“æœ
    print(f"\nğŸ“ˆ FLQé‡åŒ–æ¨¡å¼å¯¹æ¯”æ€»ç»“:")
    print("-" * 60)
    print(f"{'æ¨¡å¼':<10} {'æœ€ç»ˆæŸå¤±':<12} {'æœ€ç»ˆç²¾åº¦':<12} {'æ€»é€šä¿¡(Mbits)':<15} {'å¹³å‡å‹ç¼©æ¯”':<12}")
    print("-" * 60)
    
    comparison_data = {
        "comparison_timestamp": time.strftime("%Y-%m-%d_%H-%M-%S"),
        "modes_compared": list(modes),
        "summary": []
    }
    
    for mode, logger in results.items():
        summary = logger.get_summary_for_comparison()
        comparison_data["summary"].append(summary)
        
        print(f"{mode:<10} {summary['final_loss']:<12.4f} {summary['final_accuracy']:<12.3f} "
              f"{summary['total_communication_bits']/1e6:<15.2f} {summary['avg_compression_ratio']:<12.1f}")
    
    # ä¿å­˜å¯¹æ¯”ç»“æœåˆ°JSON
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    comparison_filepath = os.path.join("results", f"flq_modes_comparison_{timestamp}.json")
    os.makedirs("results", exist_ok=True)
    
    try:
        with open(comparison_filepath, 'w', encoding='utf-8') as f:
            json.dump(comparison_data, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ“„ å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {comparison_filepath}")
    except Exception as e:
        print(f"âŒ ä¿å­˜å¯¹æ¯”ç»“æœå¤±è´¥: {e}")
    
    # ç”Ÿæˆå®Œæ•´çš„å¯¹æ¯”å›¾è¡¨
    try:
        from plot_experiment import PlotExperiment
        plotter = PlotExperiment()
        plotter.load_all_experiments()
        plotter.load_comparison_data()
        plotter.plot_all_figures()
        print("âœ… å®Œæ•´çš„è®ºæ–‡å¯¹æ¯”å›¾è¡¨å·²ç”Ÿæˆ")
    except Exception as e:
        print(f"âš ï¸ å¯¹æ¯”å›¾è¡¨ç”Ÿæˆå¯é€‰ï¼Œè·³è¿‡: {e}")
    
    return results

# ---------------------
# ä¸»ç¨‹åºå…¥å£
# ---------------------
if __name__ == "__main__":
    print("ğŸ¯ FLQè”é‚¦å­¦ä¹ ç®€åŒ–æµ‹è¯•è„šæœ¬")
    # print("åŸºäºè®ºæ–‡ã€ŠFederated Optimal Framework with Low-bitwidth Quantizationã€‹")
    print("æ•´åˆmaster.pyå’Œworker.pyåŠŸèƒ½ï¼Œå•æ–‡ä»¶è¿è¡Œ")
    
    # é€‰æ‹©å®éªŒæ¨¡å¼
    print(f"\nè¯·é€‰æ‹©å®éªŒæ¨¡å¼:")
    print("1. å•ä¸€FLQæ¨¡å¼æµ‹è¯•")
    print("2. å¤šç§FLQæ¨¡å¼å¯¹æ¯”")
    
    # choice = input("è¯·è¾“å…¥é€‰æ‹© (1 æˆ– 2): ").strip()
    choice = "1"
    
    if choice == "1":
        # å•ä¸€æ¨¡å¼æµ‹è¯•
        print(f"\nå½“å‰FLQæ¨¡å¼: {FLQ_MODE}")
        logger = run_federated_flq_experiment(FLQ_MODE)
    
    elif choice == "2":
        # å¤šæ¨¡å¼å¯¹æ¯”æµ‹è¯•
        results = compare_flq_modes()
    
    else:
        print("è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤æ¨¡å¼è¿è¡Œ...")
        logger = run_federated_flq_experiment(FLQ_MODE)
    
    print(f"\nğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆï¼ç»“æœå·²ä¿å­˜å’Œå¯è§†åŒ–ã€‚")
